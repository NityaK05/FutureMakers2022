# FutureMakers2022

## **Day 1**
I learned about the NumPy and Pandas libraries in Python. I also learned how to create and use 2D arrays in Python. A part from that, I was able to further polish my skills in Python while refreshing my memory about this coding language. 

## **Day 2**
I learned about different types of machine learning: supervised, unsupervised, and reinforcement. A supervised machine learning model is given labels, whereas an unsupervised machine learning model is not given labels but instead uses clusters to group the given data. I also learned about Linear Regression, Decision Trees, and Random Forests, different data structures. I look forward to learning more about these concepts and other topics in the coming weeks. 

## **Day 3**
I learned more about unsupervised learning and how it works. Using clusters, patterns about the training data can be classified - unsupervised learning uses this method. 

## **Day 4**
I learned about TensorFlow and Data Preprocessing. I also learned more about training data and testing data. Training data is what is used to create the model and train it. Testing data is used to test the model and help it learn (after the inital model has been created). 

## **Day 5**
I learned about Neural Networks and how they work. A Neural Network consists of an input layer, hidden layer, and output layer (in that order). These layers are used to determine patterns in data - they function similarly to how a brain functions. I also learned about activation functions (such as ReLU, Softmax, and Sigmoid). Softmax is used for multi-class classification, and Sigmoid is used for binary classification.  

## **Day 6**
I learned about Convolutional Neural Networks (CNNs), a popular type of Neural Network. I also learned about max-pooling (and average pooling), a method to altering the dimensions of the data (/image).

## **Day 7**
I learned about Algorithmic Bias and how this may occur due to a biased dataset. Since people create these datasets, their biases or preconceived notions may also, unknowingly, be present in these datasets. To control/reduce algorithmic bias, we must make datasets diverse and include a diverse array of data. 

## **Day 8**
I learned more in detail about CNN (Convolutional Neural Network). I also learned about regulation to control overfitting and underfitting, through dropout layers and batch normalizing layers. 

## **Day 9**
I learned about regression losses; the three functions are Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Squared Logarithmic Error (MSLE). Of these, MSE and MAE are the most popular/widely used ones. MSE should not be used when there are large outliers, in this case, MAE should be used. A large loss value means that the algorithm is not accurate enough, as the real result is not enough to the predicted result. 

## **Day 10**
I learned more about activation functions - TanH, ReLU, Leaky ReLU, Sigmoid, ELU, etc. I also learned about gradients - vanishing gradients and exploding gradients. 

## **Day 11**
I learned more about the ethics of Artificial Intelligence and Machine Learning. I also learned more about algorithmic bias and feedback loops in relation to how biases spread. 
In our mentor meeting, we talked about ethical concerns in Artificial Intelligence and Machine Learning. This can include disinformation which is fake views/misinformation that can change the public opinion. This can affect oneâ€™s knowledge on a certain topic, giving them a flawed perception of this topic. 

## **Day 12**
I learned how to create an image classifier using neural networks. I also learned how to analyze these models by looking at the accuracy and if it is overfitting, underfitting, or just right. 

## **Day 13**
I learned more about underfitting and overfitting. By using regularization, underfitting and overfitting can be decreased. One must make models more complex to reduce underfitting, and one must make models less complex to reduce overfitting. 

## **Day 14**
I learned about autoencoders. They are made up of encoders and decoders. Encoders perform downsampling, so they recognize patterns and other details in the data. Decoders perform upsampling, so they use the patterns previously recognized from the old data to produce new files. 


## **Day 15**
I learned about affective computing - a form of detecting emotions in the input data, using Artificial Intelligence and Machine Learning. This can be important in determining the meaning of certain files/messages. 

## **Day 16**
I learned about Natural Language Processing (NLP). It is made up of Tokenization (which is the process of cutting down a sentence or multiple sentences into single words), Stop Word Removal (which is the process of removing certain words, such as conjunctions and prepositions, from a sentence), Stemming (which is the process of slicing the beginning or end of words that have prefixes or suffixes, to remove them), and Lemmatization (which is the process of registering the input words to a specific word(s) in the dictionary). 

## **Day 17**
I learned about GANs - another type of neural network - and how it can be used to determine patterns in input data to create a new piece of information/data - the output. 



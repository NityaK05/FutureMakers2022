{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Day_9_Introduction_to_Loss_Functions_NK.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/NityaK05/FutureMakers2022/blob/main/Day_9_Introduction_to_Loss_Functions_NK.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "861ncVuLPeyF"
      },
      "source": [
        "![image_2021-10-30_133041.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Day 9 Objectives: \n",
        "* To introduce you to loss functions. \n"
      ],
      "metadata": {
        "id": "5rokz_Xr0kDG"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCtJpzBrYWqr"
      },
      "source": [
        "# Loss Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e4fAWIkUYUyR"
      },
      "source": [
        "Loss functions define what a good prediction is and isn’t. Choosing the right loss function dictates how well your estimator (machine learning model) will be. The criteria by which an estimator is scrutinized is its performance - how accurate the model's decisions are. This calls for a way to measure how far a particular iteration of the model is from the actual values. This is where loss functions come into play.\n",
        "\n",
        "Loss functions measure how far an estimated value is from its true value. A loss function maps decisions to their associated costs. Loss functions are not fixed, they change depending on the task in hand and the goal to be met.\n",
        "\n",
        "Worth to note we can speak of different kind of loss functions: **regression loss** functions and **classification loss** functions.\n",
        "\n",
        "Regression loss function describes the difference between the values that a model is predicting and the actual values of the labels. So the loss function has a meaning on a labeled data when we compare the prediction to the label at a single point of time. This loss function is often called the error function or the error formula. Typical error functions we use for regression models are L1 and L2, Huber loss, Quantile loss, log cosh loss.\n",
        "\n",
        "**Note**: L1 loss is also know as Mean Absolute Error. L2 Loss is also know as Mean Square Error or Quadratic loss.\n",
        "\n",
        "Loss functions for classification represent the price paid for inaccuracy of predictions in classification problems (problems of identifying which category a particular observation belongs to). To name a few: log loss, focal loss, exponential loss, hinge loss, relative entropy loss and other.\n",
        "\n",
        "*Note*: While more commonly used in regression, the square loss function can be re-written and utilized for classification."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7I1Y9BQxq72l"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g6Hpicvr6XJ0"
      },
      "source": [
        "# Regression Losses\n",
        "\n",
        "Remember, in regression, the output would be a real value. We need some loss functions which compares two real values."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eMoFFw2VUR7j",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aca843f5-516b-4a85-fd4b-23103bdce649"
      },
      "source": [
        "(train_features, train_labels), (test_features, test_labels) = keras.datasets.boston_housing.load_data()\n",
        "\n",
        "# get per-feature statistics (mean, standard deviation) from the training set to normalize by\n",
        "train_mean = np.mean(train_features, axis=0)\n",
        "train_std = np.std(train_features, axis=0)\n",
        "train_features = (train_features - train_mean) / train_std"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/boston_housing.npz\n",
            "57344/57026 [==============================] - 0s 0us/step\n",
            "65536/57026 [==================================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdSFpdB6aDH"
      },
      "source": [
        "## Mean Squared Error [MSE]\n",
        "\n",
        "As the name suggests, Mean square error is measured as the average of squared difference between predictions and actual observations. It’s only concerned with the average magnitude of error irrespective of their direction. \n",
        "\n",
        "However, due to squaring, predictions which are far away from actual values are penalized heavily in comparison to less deviated predictions. Plus MSE has nice mathematical properties which makes it easier to calculate gradients.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first square the difference between the original and estimated output with $(y_i - \\hat{y}_i)^2$. Then we take sum of the squared difference for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}(y_i - \\hat{y}_i)^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w5OO5ZfoYtCJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b06697f-18c4-4bed-c027-7661746b600c"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(10, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mse',\n",
        "              metrics=['mse'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 3s 17ms/step - loss: 587.7637 - mse: 587.7637 - val_loss: 488.6973 - val_mse: 488.6973\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 565.0143 - mse: 565.0143 - val_loss: 465.5465 - val_mse: 465.5465\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 532.2006 - mse: 532.2006 - val_loss: 430.5564 - val_mse: 430.5564\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 483.8167 - mse: 483.8167 - val_loss: 377.9577 - val_mse: 377.9577\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 413.6018 - mse: 413.6018 - val_loss: 304.0029 - val_mse: 304.0029\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 319.1455 - mse: 319.1455 - val_loss: 208.6196 - val_mse: 208.6196\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 206.5318 - mse: 206.5318 - val_loss: 108.7353 - val_mse: 108.7353\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 105.1763 - mse: 105.1763 - val_loss: 43.2738 - val_mse: 43.2738\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 55.0539 - mse: 55.0539 - val_loss: 31.9262 - val_mse: 31.9262\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 48.8601 - mse: 48.8601 - val_loss: 31.4516 - val_mse: 31.4516\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 44.5852 - mse: 44.5852 - val_loss: 26.0834 - val_mse: 26.0834\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 39.8793 - mse: 39.8793 - val_loss: 24.2730 - val_mse: 24.2730\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 37.4777 - mse: 37.4777 - val_loss: 23.2722 - val_mse: 23.2722\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 35.6570 - mse: 35.6570 - val_loss: 22.3373 - val_mse: 22.3373\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 34.0067 - mse: 34.0067 - val_loss: 22.0057 - val_mse: 22.0057\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 32.6937 - mse: 32.6937 - val_loss: 21.7641 - val_mse: 21.7641\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 31.5435 - mse: 31.5435 - val_loss: 21.3564 - val_mse: 21.3564\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 30.5224 - mse: 30.5224 - val_loss: 20.9531 - val_mse: 20.9531\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 29.4519 - mse: 29.4519 - val_loss: 19.7604 - val_mse: 19.7604\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28.5697 - mse: 28.5697 - val_loss: 19.5750 - val_mse: 19.5750\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 27.6650 - mse: 27.6650 - val_loss: 19.4341 - val_mse: 19.4341\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 26.8298 - mse: 26.8298 - val_loss: 18.7577 - val_mse: 18.7577\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 26.1202 - mse: 26.1202 - val_loss: 18.1836 - val_mse: 18.1836\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 25.3588 - mse: 25.3588 - val_loss: 18.2553 - val_mse: 18.2553\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 25.1663 - mse: 25.1663 - val_loss: 18.3496 - val_mse: 18.3496\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 24.0409 - mse: 24.0409 - val_loss: 17.3289 - val_mse: 17.3289\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 23.5098 - mse: 23.5098 - val_loss: 17.0649 - val_mse: 17.0649\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 22.9940 - mse: 22.9940 - val_loss: 16.4269 - val_mse: 16.4269\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 22.3803 - mse: 22.3803 - val_loss: 16.7954 - val_mse: 16.7954\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.9933 - mse: 21.9933 - val_loss: 16.4563 - val_mse: 16.4563\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 21.4483 - mse: 21.4483 - val_loss: 15.6426 - val_mse: 15.6426\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.9416 - mse: 20.9416 - val_loss: 15.4551 - val_mse: 15.4551\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.5029 - mse: 20.5029 - val_loss: 15.1918 - val_mse: 15.1918\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 20.1059 - mse: 20.1059 - val_loss: 14.8517 - val_mse: 14.8517\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.7735 - mse: 19.7735 - val_loss: 14.7709 - val_mse: 14.7709\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.2882 - mse: 19.2882 - val_loss: 14.2392 - val_mse: 14.2392\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 19.0662 - mse: 19.0662 - val_loss: 13.9549 - val_mse: 13.9549\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 18.6125 - mse: 18.6125 - val_loss: 13.6690 - val_mse: 13.6690\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 18.3532 - mse: 18.3532 - val_loss: 13.7711 - val_mse: 13.7711\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 18.0105 - mse: 18.0105 - val_loss: 12.9339 - val_mse: 12.9339\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 17.6603 - mse: 17.6603 - val_loss: 13.0382 - val_mse: 13.0382\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.4605 - mse: 17.4605 - val_loss: 13.1218 - val_mse: 13.1218\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.0030 - mse: 17.0030 - val_loss: 12.4822 - val_mse: 12.4822\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 16.8977 - mse: 16.8977 - val_loss: 12.2363 - val_mse: 12.2363\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.5789 - mse: 16.5789 - val_loss: 12.2273 - val_mse: 12.2273\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 16.3019 - mse: 16.3019 - val_loss: 11.6937 - val_mse: 11.6937\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.9749 - mse: 15.9749 - val_loss: 11.5908 - val_mse: 11.5908\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.7872 - mse: 15.7872 - val_loss: 11.3502 - val_mse: 11.3502\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.5010 - mse: 15.5010 - val_loss: 11.1063 - val_mse: 11.1063\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 15.3709 - mse: 15.3709 - val_loss: 11.0967 - val_mse: 11.0967\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 15.0939 - mse: 15.0939 - val_loss: 10.6785 - val_mse: 10.6785\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.8654 - mse: 14.8654 - val_loss: 10.6578 - val_mse: 10.6578\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.7035 - mse: 14.7035 - val_loss: 10.5579 - val_mse: 10.5579\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.5554 - mse: 14.5554 - val_loss: 10.2782 - val_mse: 10.2782\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 14.3742 - mse: 14.3742 - val_loss: 9.9489 - val_mse: 9.9489\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.1686 - mse: 14.1686 - val_loss: 9.6870 - val_mse: 9.6870\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.2108 - mse: 14.2108 - val_loss: 9.7896 - val_mse: 9.7896\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.7593 - mse: 13.7593 - val_loss: 9.4862 - val_mse: 9.4862\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.6221 - mse: 13.6221 - val_loss: 9.2584 - val_mse: 9.2584\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.7451 - mse: 13.7451 - val_loss: 9.0006 - val_mse: 9.0006\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.3783 - mse: 13.3783 - val_loss: 9.3350 - val_mse: 9.3350\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.3628 - mse: 13.3628 - val_loss: 9.2954 - val_mse: 9.2954\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.1696 - mse: 13.1696 - val_loss: 8.9374 - val_mse: 8.9374\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.9076 - mse: 12.9076 - val_loss: 9.0355 - val_mse: 9.0355\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.0172 - mse: 13.0172 - val_loss: 8.9995 - val_mse: 8.9995\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.6757 - mse: 12.6757 - val_loss: 8.6717 - val_mse: 8.6717\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.8607 - mse: 12.8607 - val_loss: 8.4482 - val_mse: 8.4482\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.4998 - mse: 12.4998 - val_loss: 8.1211 - val_mse: 8.1211\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.3458 - mse: 12.3458 - val_loss: 8.1573 - val_mse: 8.1573\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.1639 - mse: 12.1639 - val_loss: 8.0795 - val_mse: 8.0795\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.0914 - mse: 12.0914 - val_loss: 8.1467 - val_mse: 8.1467\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.1215 - mse: 12.1215 - val_loss: 8.3308 - val_mse: 8.3308\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.8468 - mse: 11.8468 - val_loss: 7.8351 - val_mse: 7.8351\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.7861 - mse: 11.7861 - val_loss: 7.7576 - val_mse: 7.7576\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.6707 - mse: 11.6707 - val_loss: 7.9377 - val_mse: 7.9377\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.5775 - mse: 11.5775 - val_loss: 7.6607 - val_mse: 7.6607\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.6286 - mse: 11.6286 - val_loss: 7.5618 - val_mse: 7.5618\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.4442 - mse: 11.4442 - val_loss: 7.6386 - val_mse: 7.6386\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.3644 - mse: 11.3644 - val_loss: 7.5240 - val_mse: 7.5240\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.5288 - mse: 11.5288 - val_loss: 7.6872 - val_mse: 7.6872\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.2120 - mse: 11.2120 - val_loss: 7.3749 - val_mse: 7.3749\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.2263 - mse: 11.2263 - val_loss: 7.6786 - val_mse: 7.6786\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.0015 - mse: 11.0015 - val_loss: 7.6521 - val_mse: 7.6521\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.9515 - mse: 10.9515 - val_loss: 7.6329 - val_mse: 7.6329\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.9214 - mse: 10.9214 - val_loss: 7.5337 - val_mse: 7.5337\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.8346 - mse: 10.8346 - val_loss: 7.3698 - val_mse: 7.3698\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7036 - mse: 10.7036 - val_loss: 7.4559 - val_mse: 7.4559\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.6467 - mse: 10.6467 - val_loss: 7.3376 - val_mse: 7.3376\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5856 - mse: 10.5856 - val_loss: 7.2322 - val_mse: 7.2322\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5022 - mse: 10.5022 - val_loss: 7.0624 - val_mse: 7.0624\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.4535 - mse: 10.4535 - val_loss: 7.3204 - val_mse: 7.3204\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.5212 - mse: 10.5212 - val_loss: 6.9271 - val_mse: 6.9271\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3993 - mse: 10.3993 - val_loss: 7.0745 - val_mse: 7.0745\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3114 - mse: 10.3114 - val_loss: 6.9242 - val_mse: 6.9242\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.2566 - mse: 10.2566 - val_loss: 7.2200 - val_mse: 7.2200\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.1823 - mse: 10.1823 - val_loss: 7.0183 - val_mse: 7.0183\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.1160 - mse: 10.1160 - val_loss: 7.0479 - val_mse: 7.0479\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.1304 - mse: 10.1304 - val_loss: 7.4536 - val_mse: 7.4536\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9969 - mse: 9.9969 - val_loss: 7.3169 - val_mse: 7.3169\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9782 - mse: 9.9782 - val_loss: 7.3771 - val_mse: 7.3771\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9510 - mse: 9.9510 - val_loss: 7.1331 - val_mse: 7.1331\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.9777 - mse: 9.9777 - val_loss: 7.1071 - val_mse: 7.1071\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7535 - mse: 9.7535 - val_loss: 7.3299 - val_mse: 7.3299\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7497 - mse: 9.7497 - val_loss: 7.1549 - val_mse: 7.1549\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7053 - mse: 9.7053 - val_loss: 7.2974 - val_mse: 7.2974\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6597 - mse: 9.6597 - val_loss: 7.1537 - val_mse: 7.1537\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6069 - mse: 9.6069 - val_loss: 7.1372 - val_mse: 7.1372\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.6633 - mse: 9.6633 - val_loss: 7.2210 - val_mse: 7.2210\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4547 - mse: 9.4547 - val_loss: 7.1194 - val_mse: 7.1194\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4151 - mse: 9.4151 - val_loss: 7.0731 - val_mse: 7.0731\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3620 - mse: 9.3620 - val_loss: 7.1051 - val_mse: 7.1051\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3483 - mse: 9.3483 - val_loss: 7.0533 - val_mse: 7.0533\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3461 - mse: 9.3461 - val_loss: 7.0400 - val_mse: 7.0400\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2881 - mse: 9.2881 - val_loss: 7.3127 - val_mse: 7.3127\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.2482 - mse: 9.2482 - val_loss: 7.1286 - val_mse: 7.1286\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.3538 - mse: 9.3538 - val_loss: 7.2261 - val_mse: 7.2261\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1919 - mse: 9.1919 - val_loss: 7.1678 - val_mse: 7.1678\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0924 - mse: 9.0924 - val_loss: 7.0354 - val_mse: 7.0354\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1489 - mse: 9.1489 - val_loss: 7.1528 - val_mse: 7.1528\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.0217 - mse: 9.0217 - val_loss: 6.8798 - val_mse: 6.8798\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.0009 - mse: 9.0009 - val_loss: 6.7847 - val_mse: 6.7847\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9482 - mse: 8.9482 - val_loss: 6.8726 - val_mse: 6.8726\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.8882 - mse: 8.8882 - val_loss: 6.8318 - val_mse: 6.8318\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7984 - mse: 8.7984 - val_loss: 6.9604 - val_mse: 6.9604\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8651 - mse: 8.8651 - val_loss: 6.7922 - val_mse: 6.7922\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7525 - mse: 8.7525 - val_loss: 6.8454 - val_mse: 6.8454\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8162 - mse: 8.8162 - val_loss: 6.9226 - val_mse: 6.9226\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6876 - mse: 8.6876 - val_loss: 6.8541 - val_mse: 6.8541\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5991 - mse: 8.5991 - val_loss: 6.8730 - val_mse: 6.8730\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6650 - mse: 8.6650 - val_loss: 6.9181 - val_mse: 6.9181\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5677 - mse: 8.5677 - val_loss: 6.8217 - val_mse: 6.8217\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4685 - mse: 8.4685 - val_loss: 6.8590 - val_mse: 6.8590\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5328 - mse: 8.5328 - val_loss: 6.6508 - val_mse: 6.6508\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5294 - mse: 8.5294 - val_loss: 6.7731 - val_mse: 6.7731\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4538 - mse: 8.4538 - val_loss: 6.9525 - val_mse: 6.9525\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.4553 - mse: 8.4553 - val_loss: 6.9070 - val_mse: 6.9070\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3712 - mse: 8.3712 - val_loss: 6.8540 - val_mse: 6.8540\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3524 - mse: 8.3524 - val_loss: 6.8946 - val_mse: 6.8946\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2579 - mse: 8.2579 - val_loss: 6.9838 - val_mse: 6.9838\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3719 - mse: 8.3719 - val_loss: 6.7384 - val_mse: 6.7384\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2859 - mse: 8.2859 - val_loss: 6.8039 - val_mse: 6.8039\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2204 - mse: 8.2204 - val_loss: 6.4831 - val_mse: 6.4831\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2721 - mse: 8.2721 - val_loss: 6.4994 - val_mse: 6.4994\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3787 - mse: 8.3787 - val_loss: 6.7602 - val_mse: 6.7602\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 12ms/step - loss: 8.1334 - mse: 8.1334 - val_loss: 6.6100 - val_mse: 6.6100\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 14ms/step - loss: 8.1159 - mse: 8.1159 - val_loss: 6.9327 - val_mse: 6.9327\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 11ms/step - loss: 8.0001 - mse: 8.0001 - val_loss: 6.8940 - val_mse: 6.8940\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0107 - mse: 8.0107 - val_loss: 6.8609 - val_mse: 6.8609\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.0100 - mse: 8.0100 - val_loss: 6.7689 - val_mse: 6.7689\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.0991 - mse: 8.0991 - val_loss: 6.8306 - val_mse: 6.8306\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8839 - mse: 7.8839 - val_loss: 6.9310 - val_mse: 6.9310\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9933 - mse: 7.9933 - val_loss: 6.9121 - val_mse: 6.9121\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9760 - mse: 7.9760 - val_loss: 6.8487 - val_mse: 6.8487\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8366 - mse: 7.8366 - val_loss: 6.7822 - val_mse: 6.7822\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8172 - mse: 7.8172 - val_loss: 6.7632 - val_mse: 6.7632\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7734 - mse: 7.7734 - val_loss: 6.9373 - val_mse: 6.9373\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8168 - mse: 7.8168 - val_loss: 6.7739 - val_mse: 6.7739\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7423 - mse: 7.7423 - val_loss: 6.6499 - val_mse: 6.6499\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6971 - mse: 7.6971 - val_loss: 6.6757 - val_mse: 6.6757\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7518 - mse: 7.7518 - val_loss: 6.7120 - val_mse: 6.7120\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7694 - mse: 7.7694 - val_loss: 6.6378 - val_mse: 6.6378\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6419 - mse: 7.6419 - val_loss: 6.8989 - val_mse: 6.8989\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6418 - mse: 7.6418 - val_loss: 6.7872 - val_mse: 6.7872\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6258 - mse: 7.6258 - val_loss: 6.5964 - val_mse: 6.5964\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5841 - mse: 7.5841 - val_loss: 6.4520 - val_mse: 6.4520\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5842 - mse: 7.5842 - val_loss: 6.6156 - val_mse: 6.6156\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5170 - mse: 7.5170 - val_loss: 6.5865 - val_mse: 6.5865\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5568 - mse: 7.5568 - val_loss: 6.6588 - val_mse: 6.6588\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5392 - mse: 7.5392 - val_loss: 6.6292 - val_mse: 6.6292\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6273 - mse: 7.6273 - val_loss: 6.6788 - val_mse: 6.6788\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5304 - mse: 7.5304 - val_loss: 6.8137 - val_mse: 6.8137\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4794 - mse: 7.4794 - val_loss: 6.6549 - val_mse: 6.6549\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4305 - mse: 7.4305 - val_loss: 6.7137 - val_mse: 6.7137\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4330 - mse: 7.4330 - val_loss: 6.8455 - val_mse: 6.8455\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4110 - mse: 7.4110 - val_loss: 6.6567 - val_mse: 6.6567\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3895 - mse: 7.3895 - val_loss: 6.7133 - val_mse: 6.7133\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3447 - mse: 7.3447 - val_loss: 6.7589 - val_mse: 6.7589\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.3947 - mse: 7.3947 - val_loss: 6.8136 - val_mse: 6.8136\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.4030 - mse: 7.4030 - val_loss: 6.8012 - val_mse: 6.8012\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3666 - mse: 7.3666 - val_loss: 6.7467 - val_mse: 6.7467\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.3655 - mse: 7.3655 - val_loss: 6.8887 - val_mse: 6.8887\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5183 - mse: 7.5183 - val_loss: 6.6987 - val_mse: 6.6987\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1764 - mse: 7.1764 - val_loss: 6.9125 - val_mse: 6.9125\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2415 - mse: 7.2415 - val_loss: 6.8848 - val_mse: 6.8848\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2380 - mse: 7.2380 - val_loss: 6.7407 - val_mse: 6.7407\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2697 - mse: 7.2697 - val_loss: 7.0558 - val_mse: 7.0558\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2994 - mse: 7.2994 - val_loss: 6.7988 - val_mse: 6.7988\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2571 - mse: 7.2571 - val_loss: 7.1024 - val_mse: 7.1024\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1847 - mse: 7.1847 - val_loss: 6.8183 - val_mse: 6.8183\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0492 - mse: 7.0492 - val_loss: 6.6107 - val_mse: 6.6107\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0427 - mse: 7.0427 - val_loss: 6.7248 - val_mse: 6.7248\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0577 - mse: 7.0577 - val_loss: 6.8202 - val_mse: 6.8202\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1834 - mse: 7.1834 - val_loss: 6.9249 - val_mse: 6.9249\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0441 - mse: 7.0441 - val_loss: 6.9193 - val_mse: 6.9193\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0906 - mse: 7.0906 - val_loss: 6.8511 - val_mse: 6.8511\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0260 - mse: 7.0260 - val_loss: 6.8549 - val_mse: 6.8549\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2525 - mse: 7.2525 - val_loss: 6.8078 - val_mse: 6.8078\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0990 - mse: 7.0990 - val_loss: 6.8827 - val_mse: 6.8827\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0010 - mse: 7.0010 - val_loss: 6.7349 - val_mse: 6.7349\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9283 - mse: 6.9283 - val_loss: 6.8105 - val_mse: 6.8105\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9080 - mse: 6.9080 - val_loss: 6.6626 - val_mse: 6.6626\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8949 - mse: 6.8949 - val_loss: 6.8181 - val_mse: 6.8181\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8558 - mse: 6.8558 - val_loss: 6.7592 - val_mse: 6.7592\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9223 - mse: 6.9223 - val_loss: 6.5820 - val_mse: 6.5820\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7672 - mse: 6.7672 - val_loss: 6.8960 - val_mse: 6.8960\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8415 - mse: 6.8415 - val_loss: 6.8383 - val_mse: 6.8383\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9082 - mse: 6.9082 - val_loss: 6.7934 - val_mse: 6.7934\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7627 - mse: 6.7627 - val_loss: 6.9110 - val_mse: 6.9110\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7411 - mse: 6.7411 - val_loss: 6.6351 - val_mse: 6.6351\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7777 - mse: 6.7777 - val_loss: 7.0724 - val_mse: 7.0724\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6952 - mse: 6.6952 - val_loss: 6.8812 - val_mse: 6.8812\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7538 - mse: 6.7538 - val_loss: 6.8117 - val_mse: 6.8117\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7096 - mse: 6.7096 - val_loss: 6.8550 - val_mse: 6.8550\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7709 - mse: 6.7709 - val_loss: 6.8872 - val_mse: 6.8872\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6486 - mse: 6.6486 - val_loss: 6.4346 - val_mse: 6.4346\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5895 - mse: 6.5895 - val_loss: 6.6506 - val_mse: 6.6506\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6029 - mse: 6.6029 - val_loss: 6.7600 - val_mse: 6.7600\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6144 - mse: 6.6144 - val_loss: 6.6094 - val_mse: 6.6094\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6168 - mse: 6.6168 - val_loss: 6.5971 - val_mse: 6.5971\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5621 - mse: 6.5621 - val_loss: 6.7580 - val_mse: 6.7580\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6637 - mse: 6.6637 - val_loss: 6.5382 - val_mse: 6.5382\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5472 - mse: 6.5472 - val_loss: 6.7167 - val_mse: 6.7167\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5530 - mse: 6.5530 - val_loss: 6.4631 - val_mse: 6.4631\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5724 - mse: 6.5724 - val_loss: 6.7465 - val_mse: 6.7465\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5359 - mse: 6.5359 - val_loss: 6.9672 - val_mse: 6.9672\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4667 - mse: 6.4667 - val_loss: 6.7177 - val_mse: 6.7177\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4189 - mse: 6.4189 - val_loss: 6.9062 - val_mse: 6.9062\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7110 - mse: 6.7110 - val_loss: 6.6876 - val_mse: 6.6876\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4543 - mse: 6.4543 - val_loss: 6.6149 - val_mse: 6.6149\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3918 - mse: 6.3918 - val_loss: 6.6691 - val_mse: 6.6691\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4783 - mse: 6.4783 - val_loss: 6.4729 - val_mse: 6.4729\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6677 - mse: 6.6677 - val_loss: 6.5535 - val_mse: 6.5535\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4842 - mse: 6.4842 - val_loss: 6.6808 - val_mse: 6.6808\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.4169 - mse: 6.4169 - val_loss: 6.9504 - val_mse: 6.9504\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3977 - mse: 6.3977 - val_loss: 6.4576 - val_mse: 6.4576\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3244 - mse: 6.3244 - val_loss: 6.4832 - val_mse: 6.4832\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2994 - mse: 6.2994 - val_loss: 6.6279 - val_mse: 6.6279\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2739 - mse: 6.2739 - val_loss: 6.5821 - val_mse: 6.5821\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2785 - mse: 6.2785 - val_loss: 6.5569 - val_mse: 6.5569\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3317 - mse: 6.3317 - val_loss: 6.4689 - val_mse: 6.4689\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2131 - mse: 6.2131 - val_loss: 6.5008 - val_mse: 6.5008\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2405 - mse: 6.2405 - val_loss: 6.4193 - val_mse: 6.4193\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1928 - mse: 6.1928 - val_loss: 6.6156 - val_mse: 6.6156\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2337 - mse: 6.2337 - val_loss: 6.7243 - val_mse: 6.7243\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1186 - mse: 6.1186 - val_loss: 6.3112 - val_mse: 6.3112\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2347 - mse: 6.2347 - val_loss: 6.4148 - val_mse: 6.4148\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1748 - mse: 6.1748 - val_loss: 6.5517 - val_mse: 6.5517\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1730 - mse: 6.1730 - val_loss: 6.6880 - val_mse: 6.6880\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2232 - mse: 6.2232 - val_loss: 6.8998 - val_mse: 6.8998\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1791 - mse: 6.1791 - val_loss: 6.8296 - val_mse: 6.8296\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f04b00cf6d0>"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kEFaa0VQaKef"
      },
      "source": [
        "## Question 1\n",
        "\n",
        "Now that you know how MSE works, you need to plot the behavior of MSE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zc5OFsCmadXE"
      },
      "source": [
        "### Answer 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L8RF_LUDbdo2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "a8842b50-9719-49f6-ef95-66cfa5b24ab3"
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "def mse():\n",
        "  mse = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    mse[i] = np.sum(errors[i]**2)/n\n",
        "  return mse\n",
        "\n",
        "plt.plot(errors, mse(), c='#ED4F46', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Squared Errors')\n",
        "plt.show()"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aShHwxvw6hml"
      },
      "source": [
        "## Mean Absolute Error [MAE]\n",
        "\n",
        "Mean absolute error, on the other hand, is measured as the average of sum of absolute differences between predictions and actual observations. \n",
        "\n",
        "Like MSE, this as well measures the magnitude of error without considering their direction. \n",
        "\n",
        "Unlike MSE, MAE needs more complicated tools such as linear programming to compute the gradients. Plus MAE is more robust to outliers since it does not make use of square.\n",
        "\n",
        "Let's assume there are $n$ data samples, for $i^{th}$ sample; the actual output is $y_i$ and $\\hat{y}_i$ is the estimated output from the regression model. \n",
        "\n",
        "We first take the absolute difference between the original and estimated output with $|y_i - \\hat{y}_i|2$. Then we take sum of the absolute differences for all the samples. And finally divide it by the total count of samples, which is $n$. \n",
        "\n",
        "$$MSE = \\frac{\\sum_{i=1}^{n}|y_i - \\hat{y}_i|}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MI9_GEL86hmn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b27598c-2c21-4785-f6ba-a57dd3bdc138"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss='mae',\n",
        "              metrics=['mae'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 1s 15ms/step - loss: 21.8327 - mae: 21.8327 - val_loss: 20.0572 - val_mae: 20.0572\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 20.1922 - mae: 20.1922 - val_loss: 17.9308 - val_mae: 17.9308\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 17.4339 - mae: 17.4339 - val_loss: 14.5324 - val_mae: 14.5324\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 13.4104 - mae: 13.4104 - val_loss: 9.3893 - val_mae: 9.3893\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.6956 - mae: 8.6956 - val_loss: 6.6838 - val_mae: 6.6838\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0182 - mae: 7.0182 - val_loss: 5.5391 - val_mae: 5.5391\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.5581 - mae: 5.5581 - val_loss: 4.0170 - val_mae: 4.0170\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.3218 - mae: 4.3218 - val_loss: 3.7803 - val_mae: 3.7803\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3.6175 - mae: 3.6175 - val_loss: 3.4058 - val_mae: 3.4058\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 3.3031 - mae: 3.3031 - val_loss: 3.2662 - val_mae: 3.2662\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.0732 - mae: 3.0732 - val_loss: 3.0560 - val_mae: 3.0560\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.9503 - mae: 2.9503 - val_loss: 3.0400 - val_mae: 3.0400\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.7749 - mae: 2.7749 - val_loss: 2.9285 - val_mae: 2.9285\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.6883 - mae: 2.6883 - val_loss: 2.6529 - val_mae: 2.6529\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.6414 - mae: 2.6414 - val_loss: 2.7411 - val_mae: 2.7411\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.5601 - mae: 2.5601 - val_loss: 2.6395 - val_mae: 2.6395\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4833 - mae: 2.4833 - val_loss: 2.6152 - val_mae: 2.6152\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.4375 - mae: 2.4375 - val_loss: 2.5678 - val_mae: 2.5678\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.3808 - mae: 2.3808 - val_loss: 2.5683 - val_mae: 2.5683\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.3284 - mae: 2.3284 - val_loss: 2.5409 - val_mae: 2.5409\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.3358 - mae: 2.3358 - val_loss: 2.5307 - val_mae: 2.5307\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.2592 - mae: 2.2592 - val_loss: 2.3553 - val_mae: 2.3553\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.2333 - mae: 2.2333 - val_loss: 2.4207 - val_mae: 2.4207\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.2026 - mae: 2.2026 - val_loss: 2.3420 - val_mae: 2.3420\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1773 - mae: 2.1773 - val_loss: 2.4673 - val_mae: 2.4673\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1849 - mae: 2.1849 - val_loss: 2.3815 - val_mae: 2.3815\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.1315 - mae: 2.1315 - val_loss: 2.3478 - val_mae: 2.3478\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.1181 - mae: 2.1181 - val_loss: 2.3392 - val_mae: 2.3392\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0866 - mae: 2.0866 - val_loss: 2.2447 - val_mae: 2.2447\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0973 - mae: 2.0973 - val_loss: 2.3465 - val_mae: 2.3465\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0556 - mae: 2.0556 - val_loss: 2.1973 - val_mae: 2.1973\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 2.0767 - mae: 2.0767 - val_loss: 2.2952 - val_mae: 2.2952\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0227 - mae: 2.0227 - val_loss: 2.1630 - val_mae: 2.1630\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0204 - mae: 2.0204 - val_loss: 2.3250 - val_mae: 2.3250\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0216 - mae: 2.0216 - val_loss: 2.1584 - val_mae: 2.1584\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9639 - mae: 1.9639 - val_loss: 2.1454 - val_mae: 2.1454\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9600 - mae: 1.9600 - val_loss: 2.3822 - val_mae: 2.3822\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 2.0082 - mae: 2.0082 - val_loss: 2.1123 - val_mae: 2.1123\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.9308 - mae: 1.9308 - val_loss: 2.2586 - val_mae: 2.2586\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.9008 - mae: 1.9008 - val_loss: 2.2113 - val_mae: 2.2113\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8865 - mae: 1.8865 - val_loss: 2.1336 - val_mae: 2.1336\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8666 - mae: 1.8666 - val_loss: 2.1396 - val_mae: 2.1396\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8288 - mae: 1.8288 - val_loss: 2.1844 - val_mae: 2.1844\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.8574 - mae: 1.8574 - val_loss: 2.2555 - val_mae: 2.2555\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8925 - mae: 1.8925 - val_loss: 2.1158 - val_mae: 2.1158\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8474 - mae: 1.8474 - val_loss: 2.2252 - val_mae: 2.2252\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8018 - mae: 1.8018 - val_loss: 2.1005 - val_mae: 2.1005\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8194 - mae: 1.8194 - val_loss: 2.1923 - val_mae: 2.1923\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8298 - mae: 1.8298 - val_loss: 2.1590 - val_mae: 2.1590\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.8036 - mae: 1.8036 - val_loss: 2.1049 - val_mae: 2.1049\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.8063 - mae: 1.8063 - val_loss: 2.1628 - val_mae: 2.1628\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7426 - mae: 1.7426 - val_loss: 2.1336 - val_mae: 2.1336\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7329 - mae: 1.7329 - val_loss: 2.2397 - val_mae: 2.2397\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7830 - mae: 1.7830 - val_loss: 2.1246 - val_mae: 2.1246\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7403 - mae: 1.7403 - val_loss: 2.1665 - val_mae: 2.1665\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7511 - mae: 1.7511 - val_loss: 2.0985 - val_mae: 2.0985\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7305 - mae: 1.7305 - val_loss: 2.1810 - val_mae: 2.1810\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7870 - mae: 1.7870 - val_loss: 2.1760 - val_mae: 2.1760\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7210 - mae: 1.7210 - val_loss: 2.0897 - val_mae: 2.0897\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.7432 - mae: 1.7432 - val_loss: 2.1803 - val_mae: 2.1803\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7112 - mae: 1.7112 - val_loss: 2.0201 - val_mae: 2.0201\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6814 - mae: 1.6814 - val_loss: 2.1476 - val_mae: 2.1476\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6523 - mae: 1.6523 - val_loss: 2.1315 - val_mae: 2.1315\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6831 - mae: 1.6831 - val_loss: 2.0085 - val_mae: 2.0085\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6271 - mae: 1.6271 - val_loss: 2.1207 - val_mae: 2.1207\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6237 - mae: 1.6237 - val_loss: 2.0939 - val_mae: 2.0939\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6429 - mae: 1.6429 - val_loss: 2.0926 - val_mae: 2.0926\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.6133 - mae: 1.6133 - val_loss: 2.0995 - val_mae: 2.0995\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.6017 - mae: 1.6017 - val_loss: 2.1523 - val_mae: 2.1523\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5614 - mae: 1.5614 - val_loss: 2.0090 - val_mae: 2.0090\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5359 - mae: 1.5359 - val_loss: 2.1481 - val_mae: 2.1481\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5770 - mae: 1.5770 - val_loss: 2.1321 - val_mae: 2.1321\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5242 - mae: 1.5242 - val_loss: 2.1269 - val_mae: 2.1269\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5144 - mae: 1.5144 - val_loss: 2.0408 - val_mae: 2.0408\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5838 - mae: 1.5838 - val_loss: 2.1523 - val_mae: 2.1523\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5626 - mae: 1.5626 - val_loss: 2.1295 - val_mae: 2.1295\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.5949 - mae: 1.5949 - val_loss: 2.0694 - val_mae: 2.0694\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5490 - mae: 1.5490 - val_loss: 2.1204 - val_mae: 2.1204\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4816 - mae: 1.4816 - val_loss: 2.0662 - val_mae: 2.0662\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5194 - mae: 1.5194 - val_loss: 2.1549 - val_mae: 2.1549\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5025 - mae: 1.5025 - val_loss: 2.0630 - val_mae: 2.0630\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5136 - mae: 1.5136 - val_loss: 2.0427 - val_mae: 2.0427\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4791 - mae: 1.4791 - val_loss: 2.0926 - val_mae: 2.0926\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4816 - mae: 1.4816 - val_loss: 2.0475 - val_mae: 2.0475\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4960 - mae: 1.4960 - val_loss: 2.0380 - val_mae: 2.0380\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4648 - mae: 1.4648 - val_loss: 2.0995 - val_mae: 2.0995\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.4607 - mae: 1.4607 - val_loss: 2.0761 - val_mae: 2.0761\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.5127 - mae: 1.5127 - val_loss: 2.0712 - val_mae: 2.0712\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4612 - mae: 1.4612 - val_loss: 2.0882 - val_mae: 2.0882\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4366 - mae: 1.4366 - val_loss: 2.0538 - val_mae: 2.0538\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4501 - mae: 1.4501 - val_loss: 2.0496 - val_mae: 2.0496\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4397 - mae: 1.4397 - val_loss: 2.0546 - val_mae: 2.0546\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4624 - mae: 1.4624 - val_loss: 2.0605 - val_mae: 2.0605\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4310 - mae: 1.4310 - val_loss: 2.0495 - val_mae: 2.0495\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4371 - mae: 1.4371 - val_loss: 2.0477 - val_mae: 2.0477\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4365 - mae: 1.4365 - val_loss: 2.0150 - val_mae: 2.0150\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4428 - mae: 1.4428 - val_loss: 2.1794 - val_mae: 2.1794\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4872 - mae: 1.4872 - val_loss: 2.0211 - val_mae: 2.0211\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4681 - mae: 1.4681 - val_loss: 2.0451 - val_mae: 2.0451\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4308 - mae: 1.4308 - val_loss: 1.9873 - val_mae: 1.9873\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4274 - mae: 1.4274 - val_loss: 2.0027 - val_mae: 2.0027\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3989 - mae: 1.3989 - val_loss: 1.9877 - val_mae: 1.9877\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4004 - mae: 1.4004 - val_loss: 1.9737 - val_mae: 1.9737\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3955 - mae: 1.3955 - val_loss: 2.1182 - val_mae: 2.1182\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 1.4426 - mae: 1.4426 - val_loss: 2.0439 - val_mae: 2.0439\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4113 - mae: 1.4113 - val_loss: 1.9810 - val_mae: 1.9810\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4651 - mae: 1.4651 - val_loss: 1.9936 - val_mae: 1.9936\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.4257 - mae: 1.4257 - val_loss: 2.0819 - val_mae: 2.0819\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3626 - mae: 1.3626 - val_loss: 2.0042 - val_mae: 2.0042\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.4211 - mae: 1.4211 - val_loss: 1.9706 - val_mae: 1.9706\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3537 - mae: 1.3537 - val_loss: 1.9711 - val_mae: 1.9711\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3594 - mae: 1.3594 - val_loss: 2.0374 - val_mae: 2.0374\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3295 - mae: 1.3295 - val_loss: 1.9737 - val_mae: 1.9737\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.3247 - mae: 1.3247 - val_loss: 1.9543 - val_mae: 1.9543\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3233 - mae: 1.3233 - val_loss: 2.0051 - val_mae: 2.0051\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3619 - mae: 1.3619 - val_loss: 2.0126 - val_mae: 2.0126\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3385 - mae: 1.3385 - val_loss: 2.0088 - val_mae: 2.0088\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3228 - mae: 1.3228 - val_loss: 2.0325 - val_mae: 2.0325\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3257 - mae: 1.3257 - val_loss: 1.9552 - val_mae: 1.9552\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3204 - mae: 1.3204 - val_loss: 1.9665 - val_mae: 1.9665\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3411 - mae: 1.3411 - val_loss: 1.9504 - val_mae: 1.9504\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3254 - mae: 1.3254 - val_loss: 1.9797 - val_mae: 1.9797\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3165 - mae: 1.3165 - val_loss: 1.9644 - val_mae: 1.9644\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2950 - mae: 1.2950 - val_loss: 1.9355 - val_mae: 1.9355\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2921 - mae: 1.2921 - val_loss: 1.9237 - val_mae: 1.9237\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3587 - mae: 1.3587 - val_loss: 1.9001 - val_mae: 1.9001\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2982 - mae: 1.2982 - val_loss: 1.9315 - val_mae: 1.9315\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3915 - mae: 1.3915 - val_loss: 1.9143 - val_mae: 1.9143\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3248 - mae: 1.3248 - val_loss: 1.9652 - val_mae: 1.9652\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3128 - mae: 1.3128 - val_loss: 1.8588 - val_mae: 1.8588\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2830 - mae: 1.2830 - val_loss: 1.8976 - val_mae: 1.8976\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3025 - mae: 1.3025 - val_loss: 1.9040 - val_mae: 1.9040\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3560 - mae: 1.3560 - val_loss: 1.9538 - val_mae: 1.9538\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3942 - mae: 1.3942 - val_loss: 1.9476 - val_mae: 1.9476\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2872 - mae: 1.2872 - val_loss: 1.9161 - val_mae: 1.9161\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2622 - mae: 1.2622 - val_loss: 1.8938 - val_mae: 1.8938\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2418 - mae: 1.2418 - val_loss: 1.8803 - val_mae: 1.8803\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.2684 - mae: 1.2684 - val_loss: 1.8755 - val_mae: 1.8755\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2511 - mae: 1.2511 - val_loss: 1.9365 - val_mae: 1.9365\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2405 - mae: 1.2405 - val_loss: 1.8710 - val_mae: 1.8710\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3030 - mae: 1.3030 - val_loss: 1.9078 - val_mae: 1.9078\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3049 - mae: 1.3049 - val_loss: 1.9307 - val_mae: 1.9307\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2541 - mae: 1.2541 - val_loss: 1.8900 - val_mae: 1.8900\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2114 - mae: 1.2114 - val_loss: 1.8502 - val_mae: 1.8502\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2611 - mae: 1.2611 - val_loss: 1.9614 - val_mae: 1.9614\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3660 - mae: 1.3660 - val_loss: 1.8209 - val_mae: 1.8209\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2487 - mae: 1.2487 - val_loss: 1.8507 - val_mae: 1.8507\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2097 - mae: 1.2097 - val_loss: 1.9089 - val_mae: 1.9089\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2014 - mae: 1.2014 - val_loss: 1.8718 - val_mae: 1.8718\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2283 - mae: 1.2283 - val_loss: 1.8397 - val_mae: 1.8397\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2461 - mae: 1.2461 - val_loss: 1.8763 - val_mae: 1.8763\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1822 - mae: 1.1822 - val_loss: 1.9045 - val_mae: 1.9045\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1870 - mae: 1.1870 - val_loss: 1.8914 - val_mae: 1.8914\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2383 - mae: 1.2383 - val_loss: 1.8923 - val_mae: 1.8923\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3643 - mae: 1.3643 - val_loss: 1.8855 - val_mae: 1.8855\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3250 - mae: 1.3250 - val_loss: 1.8120 - val_mae: 1.8120\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2672 - mae: 1.2672 - val_loss: 1.7763 - val_mae: 1.7763\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.3389 - mae: 1.3389 - val_loss: 1.8459 - val_mae: 1.8459\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1644 - mae: 1.1644 - val_loss: 1.9603 - val_mae: 1.9603\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2430 - mae: 1.2430 - val_loss: 1.8144 - val_mae: 1.8144\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2331 - mae: 1.2331 - val_loss: 1.8707 - val_mae: 1.8707\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2074 - mae: 1.2074 - val_loss: 1.8721 - val_mae: 1.8721\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2162 - mae: 1.2162 - val_loss: 1.8853 - val_mae: 1.8853\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2417 - mae: 1.2417 - val_loss: 1.8828 - val_mae: 1.8828\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1697 - mae: 1.1697 - val_loss: 1.8404 - val_mae: 1.8404\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1215 - mae: 1.1215 - val_loss: 1.9286 - val_mae: 1.9286\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1708 - mae: 1.1708 - val_loss: 1.8212 - val_mae: 1.8212\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2205 - mae: 1.2205 - val_loss: 1.8300 - val_mae: 1.8300\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2485 - mae: 1.2485 - val_loss: 1.9933 - val_mae: 1.9933\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.3388 - mae: 1.3388 - val_loss: 1.7583 - val_mae: 1.7583\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2404 - mae: 1.2404 - val_loss: 1.8082 - val_mae: 1.8082\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2170 - mae: 1.2170 - val_loss: 1.8462 - val_mae: 1.8462\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.2543 - mae: 1.2543 - val_loss: 1.9125 - val_mae: 1.9125\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1663 - mae: 1.1663 - val_loss: 1.8824 - val_mae: 1.8824\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1291 - mae: 1.1291 - val_loss: 1.8728 - val_mae: 1.8728\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1469 - mae: 1.1469 - val_loss: 1.8952 - val_mae: 1.8952\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1400 - mae: 1.1400 - val_loss: 1.8257 - val_mae: 1.8257\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1191 - mae: 1.1191 - val_loss: 1.8274 - val_mae: 1.8274\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1117 - mae: 1.1117 - val_loss: 1.8740 - val_mae: 1.8740\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1331 - mae: 1.1331 - val_loss: 1.7807 - val_mae: 1.7807\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1807 - mae: 1.1807 - val_loss: 1.9075 - val_mae: 1.9075\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1590 - mae: 1.1590 - val_loss: 1.9696 - val_mae: 1.9696\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1383 - mae: 1.1383 - val_loss: 1.8732 - val_mae: 1.8732\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1002 - mae: 1.1002 - val_loss: 1.8021 - val_mae: 1.8021\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1504 - mae: 1.1504 - val_loss: 1.8179 - val_mae: 1.8179\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0987 - mae: 1.0987 - val_loss: 1.8768 - val_mae: 1.8768\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0780 - mae: 1.0780 - val_loss: 1.8195 - val_mae: 1.8195\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1095 - mae: 1.1095 - val_loss: 1.9192 - val_mae: 1.9192\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1397 - mae: 1.1397 - val_loss: 1.8230 - val_mae: 1.8230\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0992 - mae: 1.0992 - val_loss: 1.8109 - val_mae: 1.8109\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0344 - mae: 1.0344 - val_loss: 1.8315 - val_mae: 1.8315\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0894 - mae: 1.0894 - val_loss: 1.8437 - val_mae: 1.8437\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1073 - mae: 1.1073 - val_loss: 1.8724 - val_mae: 1.8724\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0642 - mae: 1.0642 - val_loss: 1.8393 - val_mae: 1.8393\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0435 - mae: 1.0435 - val_loss: 1.8650 - val_mae: 1.8650\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0740 - mae: 1.0740 - val_loss: 1.8701 - val_mae: 1.8701\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0763 - mae: 1.0763 - val_loss: 1.8631 - val_mae: 1.8631\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0436 - mae: 1.0436 - val_loss: 1.8967 - val_mae: 1.8967\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0266 - mae: 1.0266 - val_loss: 1.8218 - val_mae: 1.8218\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.1358 - mae: 1.1358 - val_loss: 1.9677 - val_mae: 1.9677\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0508 - mae: 1.0508 - val_loss: 1.9213 - val_mae: 1.9213\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1089 - mae: 1.1089 - val_loss: 1.9165 - val_mae: 1.9165\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0691 - mae: 1.0691 - val_loss: 1.8557 - val_mae: 1.8557\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.1189 - mae: 1.1189 - val_loss: 1.9079 - val_mae: 1.9079\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2962 - mae: 1.2962 - val_loss: 1.7849 - val_mae: 1.7849\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.2413 - mae: 1.2413 - val_loss: 1.9448 - val_mae: 1.9448\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.1144 - mae: 1.1144 - val_loss: 1.8014 - val_mae: 1.8014\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0682 - mae: 1.0682 - val_loss: 1.8208 - val_mae: 1.8208\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0433 - mae: 1.0433 - val_loss: 1.8780 - val_mae: 1.8780\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0877 - mae: 1.0877 - val_loss: 1.9609 - val_mae: 1.9609\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0938 - mae: 1.0938 - val_loss: 1.8677 - val_mae: 1.8677\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0062 - mae: 1.0062 - val_loss: 1.8612 - val_mae: 1.8612\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 1.0081 - mae: 1.0081 - val_loss: 1.8491 - val_mae: 1.8491\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0216 - mae: 1.0216 - val_loss: 1.8151 - val_mae: 1.8151\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0270 - mae: 1.0270 - val_loss: 1.8499 - val_mae: 1.8499\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0007 - mae: 1.0007 - val_loss: 1.7985 - val_mae: 1.7985\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0100 - mae: 1.0100 - val_loss: 1.8738 - val_mae: 1.8738\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9736 - mae: 0.9736 - val_loss: 1.8910 - val_mae: 1.8910\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9468 - mae: 0.9468 - val_loss: 1.8688 - val_mae: 1.8688\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0127 - mae: 1.0127 - val_loss: 1.8613 - val_mae: 1.8613\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9802 - mae: 0.9802 - val_loss: 1.8853 - val_mae: 1.8853\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.0349 - mae: 1.0349 - val_loss: 1.8357 - val_mae: 1.8357\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9971 - mae: 0.9971 - val_loss: 1.8546 - val_mae: 1.8546\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9592 - mae: 0.9592 - val_loss: 1.8619 - val_mae: 1.8619\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9662 - mae: 0.9662 - val_loss: 1.8710 - val_mae: 1.8710\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0549 - mae: 1.0549 - val_loss: 1.8011 - val_mae: 1.8011\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0124 - mae: 1.0124 - val_loss: 1.8540 - val_mae: 1.8540\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9810 - mae: 0.9810 - val_loss: 1.8644 - val_mae: 1.8644\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9586 - mae: 0.9586 - val_loss: 1.8682 - val_mae: 1.8682\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9718 - mae: 0.9718 - val_loss: 1.8584 - val_mae: 1.8584\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9573 - mae: 0.9573 - val_loss: 1.8666 - val_mae: 1.8666\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9622 - mae: 0.9622 - val_loss: 1.8793 - val_mae: 1.8793\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9303 - mae: 0.9303 - val_loss: 1.8251 - val_mae: 1.8251\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9584 - mae: 0.9584 - val_loss: 1.8372 - val_mae: 1.8372\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9413 - mae: 0.9413 - val_loss: 1.8334 - val_mae: 1.8334\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9872 - mae: 0.9872 - val_loss: 1.8583 - val_mae: 1.8583\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9832 - mae: 0.9832 - val_loss: 1.8621 - val_mae: 1.8621\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9590 - mae: 0.9590 - val_loss: 1.9056 - val_mae: 1.9056\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9945 - mae: 0.9945 - val_loss: 1.9045 - val_mae: 1.9045\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9889 - mae: 0.9889 - val_loss: 1.9522 - val_mae: 1.9522\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9482 - mae: 0.9482 - val_loss: 1.8867 - val_mae: 1.8867\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9476 - mae: 0.9476 - val_loss: 1.8398 - val_mae: 1.8398\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9801 - mae: 0.9801 - val_loss: 1.7996 - val_mae: 1.7996\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9171 - mae: 0.9171 - val_loss: 1.8344 - val_mae: 1.8344\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9250 - mae: 0.9250 - val_loss: 1.8533 - val_mae: 1.8533\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.8966 - mae: 0.8966 - val_loss: 1.8514 - val_mae: 1.8514\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.9332 - mae: 0.9332 - val_loss: 1.8274 - val_mae: 1.8274\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.9356 - mae: 0.9356 - val_loss: 1.8801 - val_mae: 1.8801\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9459 - mae: 0.9459 - val_loss: 1.8033 - val_mae: 1.8033\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.9144 - mae: 0.9144 - val_loss: 1.8048 - val_mae: 1.8048\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f043e0d4190>"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_IEl3-k3bn7N"
      },
      "source": [
        "## Question 2\n",
        "\n",
        "Now that you know how MAE works, you need to plot the behavior of MAE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HYyGYH4zbn7N"
      },
      "source": [
        "### Answer 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wK1qdGtBbn7N",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "outputId": "1f2e0960-b399-4322-fa2b-75bbec15ddc8"
      },
      "source": [
        "errors = np.arange(-5, 6)\n",
        "n = len(errors)\n",
        "\n",
        "def mae():\n",
        "  mae = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    mae[i] = np.sum(np.abs(errors[i]))/n\n",
        "  return mae\n",
        "\n",
        "plt.plot(errors, mae(), c='#0095B6', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Difference between actual and estimated ouputs')\n",
        "plt.ylabel('Mean Absolute Errors')\n",
        "plt.show()"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pIFJ88ER6s7w"
      },
      "source": [
        "## Mean Squared Logarithmic Error [MSLE]\n",
        "\n",
        "MSLE is just like MSE, but we have to take $log$ of the actual and estimated outputs because squaring and averaging. \n",
        "\n",
        "The introduction of the logarithm makes MSLE only care about the relative difference between the true and the predicted value, or in other words, it only cares about the percentual difference between them.\n",
        "\n",
        "This means that MSLE will treat small differences between small true and predicted values approximately the same as big differences between large true and predicted values.\n",
        "\n",
        "We can use MSLE when we don't want large errors to be significantly more penalized than small ones, in those cases where the range of the target value is large.\n",
        "\n",
        "*Example*: You want to predict future house prices, and your dataset includes homes that are orders of magnitude different in price. The price is a continuous value, and therefore, we want to do regression. MSLE can here be used as the loss function.\n",
        "\n",
        "$$MSLE = \\frac{\\sum_{i=1}^{n}(\\log(y_i+1) - \\log(\\hat{y}_i+1))^2}{n}$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CBkzaP9R7KnB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a0a5aa8-5b63-4240-e67a-6da00003760c"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss=tf.keras.losses.MeanSquaredLogarithmicError(),\n",
        "              metrics=['mean_squared_logarithmic_error'])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=150, validation_split = 0.1)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/150\n",
            "12/12 [==============================] - 1s 15ms/step - loss: 9.2798 - mean_squared_logarithmic_error: 9.2798 - val_loss: 8.4218 - val_mean_squared_logarithmic_error: 8.4218\n",
            "Epoch 2/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5807 - mean_squared_logarithmic_error: 7.5807 - val_loss: 6.0307 - val_mean_squared_logarithmic_error: 6.0307\n",
            "Epoch 3/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 4.8886 - mean_squared_logarithmic_error: 4.8886 - val_loss: 3.7383 - val_mean_squared_logarithmic_error: 3.7383\n",
            "Epoch 4/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 3.0138 - mean_squared_logarithmic_error: 3.0138 - val_loss: 2.2172 - val_mean_squared_logarithmic_error: 2.2172\n",
            "Epoch 5/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 1.7863 - mean_squared_logarithmic_error: 1.7863 - val_loss: 1.2690 - val_mean_squared_logarithmic_error: 1.2690\n",
            "Epoch 6/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 1.0513 - mean_squared_logarithmic_error: 1.0513 - val_loss: 0.7366 - val_mean_squared_logarithmic_error: 0.7366\n",
            "Epoch 7/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.6329 - mean_squared_logarithmic_error: 0.6329 - val_loss: 0.4478 - val_mean_squared_logarithmic_error: 0.4478\n",
            "Epoch 8/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.4050 - mean_squared_logarithmic_error: 0.4050 - val_loss: 0.2891 - val_mean_squared_logarithmic_error: 0.2891\n",
            "Epoch 9/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.2787 - mean_squared_logarithmic_error: 0.2787 - val_loss: 0.1991 - val_mean_squared_logarithmic_error: 0.1991\n",
            "Epoch 10/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.2010 - mean_squared_logarithmic_error: 0.2010 - val_loss: 0.1474 - val_mean_squared_logarithmic_error: 0.1474\n",
            "Epoch 11/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1569 - mean_squared_logarithmic_error: 0.1569 - val_loss: 0.1180 - val_mean_squared_logarithmic_error: 0.1180\n",
            "Epoch 12/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.1304 - mean_squared_logarithmic_error: 0.1304 - val_loss: 0.0999 - val_mean_squared_logarithmic_error: 0.0999\n",
            "Epoch 13/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1135 - mean_squared_logarithmic_error: 0.1135 - val_loss: 0.0876 - val_mean_squared_logarithmic_error: 0.0876\n",
            "Epoch 14/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.1019 - mean_squared_logarithmic_error: 0.1019 - val_loss: 0.0783 - val_mean_squared_logarithmic_error: 0.0783\n",
            "Epoch 15/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0922 - mean_squared_logarithmic_error: 0.0922 - val_loss: 0.0706 - val_mean_squared_logarithmic_error: 0.0706\n",
            "Epoch 16/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0847 - mean_squared_logarithmic_error: 0.0847 - val_loss: 0.0650 - val_mean_squared_logarithmic_error: 0.0650\n",
            "Epoch 17/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0789 - mean_squared_logarithmic_error: 0.0789 - val_loss: 0.0604 - val_mean_squared_logarithmic_error: 0.0604\n",
            "Epoch 18/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0741 - mean_squared_logarithmic_error: 0.0741 - val_loss: 0.0563 - val_mean_squared_logarithmic_error: 0.0563\n",
            "Epoch 19/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0693 - mean_squared_logarithmic_error: 0.0693 - val_loss: 0.0526 - val_mean_squared_logarithmic_error: 0.0526\n",
            "Epoch 20/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0653 - mean_squared_logarithmic_error: 0.0653 - val_loss: 0.0493 - val_mean_squared_logarithmic_error: 0.0493\n",
            "Epoch 21/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0616 - mean_squared_logarithmic_error: 0.0616 - val_loss: 0.0460 - val_mean_squared_logarithmic_error: 0.0460\n",
            "Epoch 22/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0580 - mean_squared_logarithmic_error: 0.0580 - val_loss: 0.0428 - val_mean_squared_logarithmic_error: 0.0428\n",
            "Epoch 23/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0548 - mean_squared_logarithmic_error: 0.0548 - val_loss: 0.0399 - val_mean_squared_logarithmic_error: 0.0399\n",
            "Epoch 24/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0518 - mean_squared_logarithmic_error: 0.0518 - val_loss: 0.0376 - val_mean_squared_logarithmic_error: 0.0376\n",
            "Epoch 25/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0491 - mean_squared_logarithmic_error: 0.0491 - val_loss: 0.0355 - val_mean_squared_logarithmic_error: 0.0355\n",
            "Epoch 26/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0467 - mean_squared_logarithmic_error: 0.0467 - val_loss: 0.0338 - val_mean_squared_logarithmic_error: 0.0338\n",
            "Epoch 27/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0446 - mean_squared_logarithmic_error: 0.0446 - val_loss: 0.0322 - val_mean_squared_logarithmic_error: 0.0322\n",
            "Epoch 28/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0427 - mean_squared_logarithmic_error: 0.0427 - val_loss: 0.0309 - val_mean_squared_logarithmic_error: 0.0309\n",
            "Epoch 29/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0410 - mean_squared_logarithmic_error: 0.0410 - val_loss: 0.0298 - val_mean_squared_logarithmic_error: 0.0298\n",
            "Epoch 30/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0394 - mean_squared_logarithmic_error: 0.0394 - val_loss: 0.0290 - val_mean_squared_logarithmic_error: 0.0290\n",
            "Epoch 31/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0378 - mean_squared_logarithmic_error: 0.0378 - val_loss: 0.0282 - val_mean_squared_logarithmic_error: 0.0282\n",
            "Epoch 32/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0365 - mean_squared_logarithmic_error: 0.0365 - val_loss: 0.0274 - val_mean_squared_logarithmic_error: 0.0274\n",
            "Epoch 33/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0356 - mean_squared_logarithmic_error: 0.0356 - val_loss: 0.0268 - val_mean_squared_logarithmic_error: 0.0268\n",
            "Epoch 34/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0343 - mean_squared_logarithmic_error: 0.0343 - val_loss: 0.0263 - val_mean_squared_logarithmic_error: 0.0263\n",
            "Epoch 35/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0333 - mean_squared_logarithmic_error: 0.0333 - val_loss: 0.0261 - val_mean_squared_logarithmic_error: 0.0261\n",
            "Epoch 36/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0325 - mean_squared_logarithmic_error: 0.0325 - val_loss: 0.0260 - val_mean_squared_logarithmic_error: 0.0260\n",
            "Epoch 37/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0317 - mean_squared_logarithmic_error: 0.0317 - val_loss: 0.0255 - val_mean_squared_logarithmic_error: 0.0255\n",
            "Epoch 38/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0309 - mean_squared_logarithmic_error: 0.0309 - val_loss: 0.0252 - val_mean_squared_logarithmic_error: 0.0252\n",
            "Epoch 39/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0303 - mean_squared_logarithmic_error: 0.0303 - val_loss: 0.0253 - val_mean_squared_logarithmic_error: 0.0253\n",
            "Epoch 40/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0297 - mean_squared_logarithmic_error: 0.0297 - val_loss: 0.0246 - val_mean_squared_logarithmic_error: 0.0246\n",
            "Epoch 41/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0292 - mean_squared_logarithmic_error: 0.0292 - val_loss: 0.0245 - val_mean_squared_logarithmic_error: 0.0245\n",
            "Epoch 42/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0287 - mean_squared_logarithmic_error: 0.0287 - val_loss: 0.0248 - val_mean_squared_logarithmic_error: 0.0248\n",
            "Epoch 43/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0282 - mean_squared_logarithmic_error: 0.0282 - val_loss: 0.0245 - val_mean_squared_logarithmic_error: 0.0245\n",
            "Epoch 44/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0278 - mean_squared_logarithmic_error: 0.0278 - val_loss: 0.0243 - val_mean_squared_logarithmic_error: 0.0243\n",
            "Epoch 45/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0275 - mean_squared_logarithmic_error: 0.0275 - val_loss: 0.0249 - val_mean_squared_logarithmic_error: 0.0249\n",
            "Epoch 46/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0271 - mean_squared_logarithmic_error: 0.0271 - val_loss: 0.0245 - val_mean_squared_logarithmic_error: 0.0245\n",
            "Epoch 47/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0267 - mean_squared_logarithmic_error: 0.0267 - val_loss: 0.0246 - val_mean_squared_logarithmic_error: 0.0246\n",
            "Epoch 48/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0264 - mean_squared_logarithmic_error: 0.0264 - val_loss: 0.0240 - val_mean_squared_logarithmic_error: 0.0240\n",
            "Epoch 49/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0261 - mean_squared_logarithmic_error: 0.0261 - val_loss: 0.0234 - val_mean_squared_logarithmic_error: 0.0234\n",
            "Epoch 50/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0260 - mean_squared_logarithmic_error: 0.0260 - val_loss: 0.0242 - val_mean_squared_logarithmic_error: 0.0242\n",
            "Epoch 51/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0255 - mean_squared_logarithmic_error: 0.0255 - val_loss: 0.0242 - val_mean_squared_logarithmic_error: 0.0242\n",
            "Epoch 52/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0253 - mean_squared_logarithmic_error: 0.0253 - val_loss: 0.0243 - val_mean_squared_logarithmic_error: 0.0243\n",
            "Epoch 53/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0251 - mean_squared_logarithmic_error: 0.0251 - val_loss: 0.0240 - val_mean_squared_logarithmic_error: 0.0240\n",
            "Epoch 54/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0248 - mean_squared_logarithmic_error: 0.0248 - val_loss: 0.0237 - val_mean_squared_logarithmic_error: 0.0237\n",
            "Epoch 55/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0246 - mean_squared_logarithmic_error: 0.0246 - val_loss: 0.0234 - val_mean_squared_logarithmic_error: 0.0234\n",
            "Epoch 56/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0244 - mean_squared_logarithmic_error: 0.0244 - val_loss: 0.0235 - val_mean_squared_logarithmic_error: 0.0235\n",
            "Epoch 57/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0242 - mean_squared_logarithmic_error: 0.0242 - val_loss: 0.0235 - val_mean_squared_logarithmic_error: 0.0235\n",
            "Epoch 58/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0240 - mean_squared_logarithmic_error: 0.0240 - val_loss: 0.0235 - val_mean_squared_logarithmic_error: 0.0235\n",
            "Epoch 59/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0238 - mean_squared_logarithmic_error: 0.0238 - val_loss: 0.0239 - val_mean_squared_logarithmic_error: 0.0239\n",
            "Epoch 60/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0237 - mean_squared_logarithmic_error: 0.0237 - val_loss: 0.0233 - val_mean_squared_logarithmic_error: 0.0233\n",
            "Epoch 61/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0234 - mean_squared_logarithmic_error: 0.0234 - val_loss: 0.0233 - val_mean_squared_logarithmic_error: 0.0233\n",
            "Epoch 62/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0232 - mean_squared_logarithmic_error: 0.0232 - val_loss: 0.0235 - val_mean_squared_logarithmic_error: 0.0235\n",
            "Epoch 63/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0232 - mean_squared_logarithmic_error: 0.0232 - val_loss: 0.0239 - val_mean_squared_logarithmic_error: 0.0239\n",
            "Epoch 64/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0229 - mean_squared_logarithmic_error: 0.0229 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 65/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0229 - mean_squared_logarithmic_error: 0.0229 - val_loss: 0.0225 - val_mean_squared_logarithmic_error: 0.0225\n",
            "Epoch 66/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0226 - mean_squared_logarithmic_error: 0.0226 - val_loss: 0.0233 - val_mean_squared_logarithmic_error: 0.0233\n",
            "Epoch 67/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0224 - mean_squared_logarithmic_error: 0.0224 - val_loss: 0.0229 - val_mean_squared_logarithmic_error: 0.0229\n",
            "Epoch 68/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0223 - mean_squared_logarithmic_error: 0.0223 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 69/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0221 - mean_squared_logarithmic_error: 0.0221 - val_loss: 0.0228 - val_mean_squared_logarithmic_error: 0.0228\n",
            "Epoch 70/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0220 - mean_squared_logarithmic_error: 0.0220 - val_loss: 0.0224 - val_mean_squared_logarithmic_error: 0.0224\n",
            "Epoch 71/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0219 - mean_squared_logarithmic_error: 0.0219 - val_loss: 0.0224 - val_mean_squared_logarithmic_error: 0.0224\n",
            "Epoch 72/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0217 - mean_squared_logarithmic_error: 0.0217 - val_loss: 0.0224 - val_mean_squared_logarithmic_error: 0.0224\n",
            "Epoch 73/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0216 - mean_squared_logarithmic_error: 0.0216 - val_loss: 0.0223 - val_mean_squared_logarithmic_error: 0.0223\n",
            "Epoch 74/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0215 - mean_squared_logarithmic_error: 0.0215 - val_loss: 0.0223 - val_mean_squared_logarithmic_error: 0.0223\n",
            "Epoch 75/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0214 - mean_squared_logarithmic_error: 0.0214 - val_loss: 0.0217 - val_mean_squared_logarithmic_error: 0.0217\n",
            "Epoch 76/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0218 - mean_squared_logarithmic_error: 0.0218 - val_loss: 0.0227 - val_mean_squared_logarithmic_error: 0.0227\n",
            "Epoch 77/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0211 - mean_squared_logarithmic_error: 0.0211 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 78/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 79/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0210 - mean_squared_logarithmic_error: 0.0210 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 80/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0208 - mean_squared_logarithmic_error: 0.0208 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 81/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0206 - mean_squared_logarithmic_error: 0.0206 - val_loss: 0.0226 - val_mean_squared_logarithmic_error: 0.0226\n",
            "Epoch 82/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0206 - mean_squared_logarithmic_error: 0.0206 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 83/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0205 - mean_squared_logarithmic_error: 0.0205 - val_loss: 0.0209 - val_mean_squared_logarithmic_error: 0.0209\n",
            "Epoch 84/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0203 - mean_squared_logarithmic_error: 0.0203 - val_loss: 0.0216 - val_mean_squared_logarithmic_error: 0.0216\n",
            "Epoch 85/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0202 - mean_squared_logarithmic_error: 0.0202 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 86/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0201 - mean_squared_logarithmic_error: 0.0201 - val_loss: 0.0211 - val_mean_squared_logarithmic_error: 0.0211\n",
            "Epoch 87/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0212 - val_mean_squared_logarithmic_error: 0.0212\n",
            "Epoch 88/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0214 - val_mean_squared_logarithmic_error: 0.0214\n",
            "Epoch 89/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0199 - mean_squared_logarithmic_error: 0.0199 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 90/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0197 - mean_squared_logarithmic_error: 0.0197 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 91/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0196 - mean_squared_logarithmic_error: 0.0196 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 92/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0195 - mean_squared_logarithmic_error: 0.0195 - val_loss: 0.0206 - val_mean_squared_logarithmic_error: 0.0206\n",
            "Epoch 93/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0194 - mean_squared_logarithmic_error: 0.0194 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 94/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0192 - mean_squared_logarithmic_error: 0.0192 - val_loss: 0.0204 - val_mean_squared_logarithmic_error: 0.0204\n",
            "Epoch 95/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0193 - mean_squared_logarithmic_error: 0.0193 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 96/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 97/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 98/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0190 - mean_squared_logarithmic_error: 0.0190 - val_loss: 0.0210 - val_mean_squared_logarithmic_error: 0.0210\n",
            "Epoch 99/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0190 - mean_squared_logarithmic_error: 0.0190 - val_loss: 0.0208 - val_mean_squared_logarithmic_error: 0.0208\n",
            "Epoch 100/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0191 - mean_squared_logarithmic_error: 0.0191 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 101/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0187 - mean_squared_logarithmic_error: 0.0187 - val_loss: 0.0215 - val_mean_squared_logarithmic_error: 0.0215\n",
            "Epoch 102/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0188 - mean_squared_logarithmic_error: 0.0188 - val_loss: 0.0213 - val_mean_squared_logarithmic_error: 0.0213\n",
            "Epoch 103/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0186 - mean_squared_logarithmic_error: 0.0186 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 104/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0184 - mean_squared_logarithmic_error: 0.0184 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 105/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0184 - mean_squared_logarithmic_error: 0.0184 - val_loss: 0.0207 - val_mean_squared_logarithmic_error: 0.0207\n",
            "Epoch 106/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0183 - mean_squared_logarithmic_error: 0.0183 - val_loss: 0.0199 - val_mean_squared_logarithmic_error: 0.0199\n",
            "Epoch 107/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 108/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0182 - mean_squared_logarithmic_error: 0.0182 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 109/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0181 - mean_squared_logarithmic_error: 0.0181 - val_loss: 0.0198 - val_mean_squared_logarithmic_error: 0.0198\n",
            "Epoch 110/150\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0202 - val_mean_squared_logarithmic_error: 0.0202\n",
            "Epoch 111/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0180 - mean_squared_logarithmic_error: 0.0180 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 112/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0179 - mean_squared_logarithmic_error: 0.0179 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 113/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - mean_squared_logarithmic_error: 0.0178 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 114/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0178 - mean_squared_logarithmic_error: 0.0178 - val_loss: 0.0201 - val_mean_squared_logarithmic_error: 0.0201\n",
            "Epoch 115/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 116/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0177 - mean_squared_logarithmic_error: 0.0177 - val_loss: 0.0203 - val_mean_squared_logarithmic_error: 0.0203\n",
            "Epoch 117/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0175 - mean_squared_logarithmic_error: 0.0175 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 118/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0176 - mean_squared_logarithmic_error: 0.0176 - val_loss: 0.0197 - val_mean_squared_logarithmic_error: 0.0197\n",
            "Epoch 119/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 120/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0174 - mean_squared_logarithmic_error: 0.0174 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 121/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - mean_squared_logarithmic_error: 0.0173 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 122/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0173 - mean_squared_logarithmic_error: 0.0173 - val_loss: 0.0196 - val_mean_squared_logarithmic_error: 0.0196\n",
            "Epoch 123/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0172 - mean_squared_logarithmic_error: 0.0172 - val_loss: 0.0200 - val_mean_squared_logarithmic_error: 0.0200\n",
            "Epoch 124/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 125/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 126/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0171 - mean_squared_logarithmic_error: 0.0171 - val_loss: 0.0195 - val_mean_squared_logarithmic_error: 0.0195\n",
            "Epoch 127/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0170 - mean_squared_logarithmic_error: 0.0170 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 128/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0169 - mean_squared_logarithmic_error: 0.0169 - val_loss: 0.0194 - val_mean_squared_logarithmic_error: 0.0194\n",
            "Epoch 129/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0168 - mean_squared_logarithmic_error: 0.0168 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 130/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0167 - mean_squared_logarithmic_error: 0.0167 - val_loss: 0.0187 - val_mean_squared_logarithmic_error: 0.0187\n",
            "Epoch 131/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 132/150\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0192 - val_mean_squared_logarithmic_error: 0.0192\n",
            "Epoch 133/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0191 - val_mean_squared_logarithmic_error: 0.0191\n",
            "Epoch 134/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0166 - mean_squared_logarithmic_error: 0.0166 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 135/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 136/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0164 - mean_squared_logarithmic_error: 0.0164 - val_loss: 0.0190 - val_mean_squared_logarithmic_error: 0.0190\n",
            "Epoch 137/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0165 - mean_squared_logarithmic_error: 0.0165 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 138/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0163 - mean_squared_logarithmic_error: 0.0163 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 139/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0188 - val_mean_squared_logarithmic_error: 0.0188\n",
            "Epoch 140/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0161 - mean_squared_logarithmic_error: 0.0161 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 141/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0162 - mean_squared_logarithmic_error: 0.0162 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 142/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 143/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0184 - val_mean_squared_logarithmic_error: 0.0184\n",
            "Epoch 144/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0186 - val_mean_squared_logarithmic_error: 0.0186\n",
            "Epoch 145/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0160 - mean_squared_logarithmic_error: 0.0160 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 146/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0159 - mean_squared_logarithmic_error: 0.0159 - val_loss: 0.0185 - val_mean_squared_logarithmic_error: 0.0185\n",
            "Epoch 147/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0182 - val_mean_squared_logarithmic_error: 0.0182\n",
            "Epoch 148/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n",
            "Epoch 149/150\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0183 - val_mean_squared_logarithmic_error: 0.0183\n",
            "Epoch 150/150\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 0.0158 - mean_squared_logarithmic_error: 0.0158 - val_loss: 0.0181 - val_mean_squared_logarithmic_error: 0.0181\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0429f963d0>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gc_jN5XwcBu9"
      },
      "source": [
        "## Question 3\n",
        "\n",
        "Now that you know how MSLE works, you need to plot the behavior of MSLE for the synthetic errors given.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_USeo3P7cBu-"
      },
      "source": [
        "### Answer 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HHr_XxgUcBu-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 281
        },
        "outputId": "02324d69-0add-4db5-8229-6faaccb46faf"
      },
      "source": [
        "actual_outputs = np.arange(0, 51)\n",
        "n = len(actual_outputs)\n",
        "estimated_outputs = np.zeros(n)\n",
        "\n",
        "def msle():\n",
        "  msle = np.zeros(n)\n",
        "  for i in range(n):\n",
        "    msle[i] = np.sum(np.log(actual_outputs[i]+1) - (np.log(estimated_outputs[i]+1)**2))/n\n",
        "  return msle\n",
        "\n",
        "plt.plot(actual_outputs, msle(), c='#F47789', marker='o')\n",
        "plt.grid()\n",
        "plt.xlabel('Actual ouputs')\n",
        "plt.ylabel('Mean Squared Logarithmic Errors')\n",
        "plt.show()"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3BylzTZ1W9Ma"
      },
      "source": [
        "## Question 4\n",
        "\n",
        "Why do we add $1$ to the outputs before passing it through $\\log()$? \n",
        "\n",
        "< We add 1 to the outputs before passing it through log() to make sure that the function/graph is never undefined (since log(0) is undefined). >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jaQiO_XeYTjE"
      },
      "source": [
        "## Question 5\n",
        "\n",
        "Write your observations about MSE, MAE, and MSLE; and compare the results achieved with all 3 loss functions. \n",
        "\n",
        "< The three functions are Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Squared Logarithmic Error (MSLE). Of these, MSE and MAE are the most popular/widely used ones. MSE should not be used when there are large outliers, in this case, MAE should be used. A large loss value means that the algorithm is not accurate enough, as the real result is not close enough to the predicted result. >"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efp4KP5GfDL7"
      },
      "source": [
        "## Question 6\n",
        "\n",
        "Plug-in any of the loss functions from [TensorFlow](https://www.tensorflow.org/api_docs/python/tf/keras/losses) docs to the `model.compile` method and see if the difference in model performance as compared to MSE, MAE, and MSLE.\n",
        "\n",
        "< I used Mean Absolute Percentage Error (MAPE), which seems to have a decreasing loss value as time progresses. However, with the high numbers, it still seems that Mean Squared Error (MSE), Mean Absolute Error (MAE), and Mean Squared Logarithmic Error (MSLE) are better functions to use in general. >"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MDWlkms1flkZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6fcd3669-4e4f-47c1-c2ba-bc968030d19a"
      },
      "source": [
        "model = keras.Sequential([\n",
        "        tf.keras.layers.Dense(100, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(50, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(20, activation='relu', input_shape=[len(train_features[0])]),\n",
        "        tf.keras.layers.Dense(1)\n",
        "    ])\n",
        "\n",
        "model.compile(optimizer='adam', \n",
        "              loss= tf.keras.losses.MeanAbsolutePercentageError(), \n",
        "              metrics=[\"mean_absolute_percentage_error\"])\n",
        "\n",
        "model.fit(train_features, train_labels, epochs=250, validation_split = 0.1)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/250\n",
            "12/12 [==============================] - 1s 16ms/step - loss: 97.7098 - mean_absolute_percentage_error: 97.7098 - val_loss: 94.6363 - val_mean_absolute_percentage_error: 94.6363\n",
            "Epoch 2/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 92.2106 - mean_absolute_percentage_error: 92.2106 - val_loss: 87.2292 - val_mean_absolute_percentage_error: 87.2292\n",
            "Epoch 3/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 83.2625 - mean_absolute_percentage_error: 83.2625 - val_loss: 77.5435 - val_mean_absolute_percentage_error: 77.5435\n",
            "Epoch 4/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 70.0737 - mean_absolute_percentage_error: 70.0737 - val_loss: 64.5222 - val_mean_absolute_percentage_error: 64.5222\n",
            "Epoch 5/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 56.1587 - mean_absolute_percentage_error: 56.1587 - val_loss: 47.7956 - val_mean_absolute_percentage_error: 47.7956\n",
            "Epoch 6/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 38.3175 - mean_absolute_percentage_error: 38.3175 - val_loss: 32.7360 - val_mean_absolute_percentage_error: 32.7360\n",
            "Epoch 7/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 28.8554 - mean_absolute_percentage_error: 28.8554 - val_loss: 27.7380 - val_mean_absolute_percentage_error: 27.7380\n",
            "Epoch 8/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 23.7868 - mean_absolute_percentage_error: 23.7868 - val_loss: 20.9262 - val_mean_absolute_percentage_error: 20.9262\n",
            "Epoch 9/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 19.4922 - mean_absolute_percentage_error: 19.4922 - val_loss: 17.9026 - val_mean_absolute_percentage_error: 17.9026\n",
            "Epoch 10/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 16.5853 - mean_absolute_percentage_error: 16.5853 - val_loss: 17.5750 - val_mean_absolute_percentage_error: 17.5750\n",
            "Epoch 11/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.8815 - mean_absolute_percentage_error: 14.8815 - val_loss: 16.3134 - val_mean_absolute_percentage_error: 16.3134\n",
            "Epoch 12/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 14.0335 - mean_absolute_percentage_error: 14.0335 - val_loss: 15.5652 - val_mean_absolute_percentage_error: 15.5652\n",
            "Epoch 13/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 13.4777 - mean_absolute_percentage_error: 13.4777 - val_loss: 15.1518 - val_mean_absolute_percentage_error: 15.1518\n",
            "Epoch 14/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 12.9250 - mean_absolute_percentage_error: 12.9250 - val_loss: 15.1491 - val_mean_absolute_percentage_error: 15.1491\n",
            "Epoch 15/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.5183 - mean_absolute_percentage_error: 12.5183 - val_loss: 14.2281 - val_mean_absolute_percentage_error: 14.2281\n",
            "Epoch 16/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 12.0163 - mean_absolute_percentage_error: 12.0163 - val_loss: 14.6996 - val_mean_absolute_percentage_error: 14.6996\n",
            "Epoch 17/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.6795 - mean_absolute_percentage_error: 11.6795 - val_loss: 13.7070 - val_mean_absolute_percentage_error: 13.7070\n",
            "Epoch 18/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.3878 - mean_absolute_percentage_error: 11.3878 - val_loss: 13.9448 - val_mean_absolute_percentage_error: 13.9448\n",
            "Epoch 19/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 11.3055 - mean_absolute_percentage_error: 11.3055 - val_loss: 13.5205 - val_mean_absolute_percentage_error: 13.5205\n",
            "Epoch 20/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 11.3376 - mean_absolute_percentage_error: 11.3376 - val_loss: 13.8503 - val_mean_absolute_percentage_error: 13.8503\n",
            "Epoch 21/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.7040 - mean_absolute_percentage_error: 10.7040 - val_loss: 13.0883 - val_mean_absolute_percentage_error: 13.0883\n",
            "Epoch 22/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 10.4372 - mean_absolute_percentage_error: 10.4372 - val_loss: 13.1542 - val_mean_absolute_percentage_error: 13.1542\n",
            "Epoch 23/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.3150 - mean_absolute_percentage_error: 10.3150 - val_loss: 12.9690 - val_mean_absolute_percentage_error: 12.9690\n",
            "Epoch 24/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 10.2414 - mean_absolute_percentage_error: 10.2414 - val_loss: 12.9940 - val_mean_absolute_percentage_error: 12.9940\n",
            "Epoch 25/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 10.0746 - mean_absolute_percentage_error: 10.0746 - val_loss: 12.6299 - val_mean_absolute_percentage_error: 12.6299\n",
            "Epoch 26/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.9886 - mean_absolute_percentage_error: 9.9886 - val_loss: 12.4200 - val_mean_absolute_percentage_error: 12.4200\n",
            "Epoch 27/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9595 - mean_absolute_percentage_error: 9.9595 - val_loss: 12.9807 - val_mean_absolute_percentage_error: 12.9807\n",
            "Epoch 28/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.9154 - mean_absolute_percentage_error: 9.9154 - val_loss: 12.5746 - val_mean_absolute_percentage_error: 12.5746\n",
            "Epoch 29/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.8595 - mean_absolute_percentage_error: 9.8595 - val_loss: 12.8538 - val_mean_absolute_percentage_error: 12.8538\n",
            "Epoch 30/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.7209 - mean_absolute_percentage_error: 9.7209 - val_loss: 12.5053 - val_mean_absolute_percentage_error: 12.5053\n",
            "Epoch 31/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.9408 - mean_absolute_percentage_error: 9.9408 - val_loss: 12.8046 - val_mean_absolute_percentage_error: 12.8046\n",
            "Epoch 32/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.8299 - mean_absolute_percentage_error: 9.8299 - val_loss: 12.5143 - val_mean_absolute_percentage_error: 12.5143\n",
            "Epoch 33/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 9.5947 - mean_absolute_percentage_error: 9.5947 - val_loss: 12.2126 - val_mean_absolute_percentage_error: 12.2126\n",
            "Epoch 34/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4764 - mean_absolute_percentage_error: 9.4764 - val_loss: 12.7045 - val_mean_absolute_percentage_error: 12.7045\n",
            "Epoch 35/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3794 - mean_absolute_percentage_error: 9.3794 - val_loss: 12.0974 - val_mean_absolute_percentage_error: 12.0974\n",
            "Epoch 36/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.4481 - mean_absolute_percentage_error: 9.4481 - val_loss: 13.0500 - val_mean_absolute_percentage_error: 13.0500\n",
            "Epoch 37/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3772 - mean_absolute_percentage_error: 9.3772 - val_loss: 12.4310 - val_mean_absolute_percentage_error: 12.4310\n",
            "Epoch 38/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.3947 - mean_absolute_percentage_error: 9.3947 - val_loss: 12.4800 - val_mean_absolute_percentage_error: 12.4800\n",
            "Epoch 39/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1692 - mean_absolute_percentage_error: 9.1692 - val_loss: 12.3386 - val_mean_absolute_percentage_error: 12.3386\n",
            "Epoch 40/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.0613 - mean_absolute_percentage_error: 9.0613 - val_loss: 12.3660 - val_mean_absolute_percentage_error: 12.3660\n",
            "Epoch 41/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1538 - mean_absolute_percentage_error: 9.1538 - val_loss: 12.2175 - val_mean_absolute_percentage_error: 12.2175\n",
            "Epoch 42/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.1058 - mean_absolute_percentage_error: 9.1058 - val_loss: 12.1742 - val_mean_absolute_percentage_error: 12.1742\n",
            "Epoch 43/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 9.1009 - mean_absolute_percentage_error: 9.1009 - val_loss: 12.3819 - val_mean_absolute_percentage_error: 12.3819\n",
            "Epoch 44/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.0681 - mean_absolute_percentage_error: 9.0681 - val_loss: 11.9821 - val_mean_absolute_percentage_error: 11.9821\n",
            "Epoch 45/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9976 - mean_absolute_percentage_error: 8.9976 - val_loss: 12.4664 - val_mean_absolute_percentage_error: 12.4664\n",
            "Epoch 46/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9523 - mean_absolute_percentage_error: 8.9523 - val_loss: 11.7745 - val_mean_absolute_percentage_error: 11.7745\n",
            "Epoch 47/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 9.0671 - mean_absolute_percentage_error: 9.0671 - val_loss: 12.3020 - val_mean_absolute_percentage_error: 12.3020\n",
            "Epoch 48/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.9272 - mean_absolute_percentage_error: 8.9272 - val_loss: 11.9118 - val_mean_absolute_percentage_error: 11.9118\n",
            "Epoch 49/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7339 - mean_absolute_percentage_error: 8.7339 - val_loss: 12.3348 - val_mean_absolute_percentage_error: 12.3348\n",
            "Epoch 50/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8771 - mean_absolute_percentage_error: 8.8771 - val_loss: 12.2492 - val_mean_absolute_percentage_error: 12.2492\n",
            "Epoch 51/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 8.7757 - mean_absolute_percentage_error: 8.7757 - val_loss: 11.9264 - val_mean_absolute_percentage_error: 11.9264\n",
            "Epoch 52/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.8119 - mean_absolute_percentage_error: 8.8119 - val_loss: 11.8557 - val_mean_absolute_percentage_error: 11.8557\n",
            "Epoch 53/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.7779 - mean_absolute_percentage_error: 8.7779 - val_loss: 12.1181 - val_mean_absolute_percentage_error: 12.1181\n",
            "Epoch 54/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.6518 - mean_absolute_percentage_error: 8.6518 - val_loss: 12.3361 - val_mean_absolute_percentage_error: 12.3361\n",
            "Epoch 55/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5433 - mean_absolute_percentage_error: 8.5433 - val_loss: 11.8938 - val_mean_absolute_percentage_error: 11.8938\n",
            "Epoch 56/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5825 - mean_absolute_percentage_error: 8.5825 - val_loss: 12.5847 - val_mean_absolute_percentage_error: 12.5847\n",
            "Epoch 57/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5776 - mean_absolute_percentage_error: 8.5776 - val_loss: 11.8064 - val_mean_absolute_percentage_error: 11.8064\n",
            "Epoch 58/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.4782 - mean_absolute_percentage_error: 8.4782 - val_loss: 12.4199 - val_mean_absolute_percentage_error: 12.4199\n",
            "Epoch 59/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.5618 - mean_absolute_percentage_error: 8.5618 - val_loss: 11.9658 - val_mean_absolute_percentage_error: 11.9658\n",
            "Epoch 60/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.5134 - mean_absolute_percentage_error: 8.5134 - val_loss: 12.2256 - val_mean_absolute_percentage_error: 12.2256\n",
            "Epoch 61/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3897 - mean_absolute_percentage_error: 8.3897 - val_loss: 12.2050 - val_mean_absolute_percentage_error: 12.2050\n",
            "Epoch 62/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3013 - mean_absolute_percentage_error: 8.3013 - val_loss: 11.8243 - val_mean_absolute_percentage_error: 11.8243\n",
            "Epoch 63/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1963 - mean_absolute_percentage_error: 8.1963 - val_loss: 12.0724 - val_mean_absolute_percentage_error: 12.0724\n",
            "Epoch 64/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.2557 - mean_absolute_percentage_error: 8.2557 - val_loss: 11.7842 - val_mean_absolute_percentage_error: 11.7842\n",
            "Epoch 65/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1618 - mean_absolute_percentage_error: 8.1618 - val_loss: 11.8419 - val_mean_absolute_percentage_error: 11.8419\n",
            "Epoch 66/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.1509 - mean_absolute_percentage_error: 8.1509 - val_loss: 11.9874 - val_mean_absolute_percentage_error: 11.9874\n",
            "Epoch 67/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.3038 - mean_absolute_percentage_error: 8.3038 - val_loss: 11.7620 - val_mean_absolute_percentage_error: 11.7620\n",
            "Epoch 68/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.0461 - mean_absolute_percentage_error: 8.0461 - val_loss: 11.9947 - val_mean_absolute_percentage_error: 11.9947\n",
            "Epoch 69/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1066 - mean_absolute_percentage_error: 8.1066 - val_loss: 12.0351 - val_mean_absolute_percentage_error: 12.0351\n",
            "Epoch 70/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.3570 - mean_absolute_percentage_error: 8.3570 - val_loss: 11.8013 - val_mean_absolute_percentage_error: 11.8013\n",
            "Epoch 71/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.2452 - mean_absolute_percentage_error: 8.2452 - val_loss: 12.3859 - val_mean_absolute_percentage_error: 12.3859\n",
            "Epoch 72/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1564 - mean_absolute_percentage_error: 8.1564 - val_loss: 11.8583 - val_mean_absolute_percentage_error: 11.8583\n",
            "Epoch 73/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9655 - mean_absolute_percentage_error: 7.9655 - val_loss: 11.8515 - val_mean_absolute_percentage_error: 11.8515\n",
            "Epoch 74/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9280 - mean_absolute_percentage_error: 7.9280 - val_loss: 12.0433 - val_mean_absolute_percentage_error: 12.0433\n",
            "Epoch 75/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 8.1138 - mean_absolute_percentage_error: 8.1138 - val_loss: 11.7519 - val_mean_absolute_percentage_error: 11.7519\n",
            "Epoch 76/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 8.1141 - mean_absolute_percentage_error: 8.1141 - val_loss: 12.0051 - val_mean_absolute_percentage_error: 12.0051\n",
            "Epoch 77/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9110 - mean_absolute_percentage_error: 7.9110 - val_loss: 12.0278 - val_mean_absolute_percentage_error: 12.0278\n",
            "Epoch 78/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.9783 - mean_absolute_percentage_error: 7.9783 - val_loss: 11.7324 - val_mean_absolute_percentage_error: 11.7324\n",
            "Epoch 79/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.9527 - mean_absolute_percentage_error: 7.9527 - val_loss: 11.8411 - val_mean_absolute_percentage_error: 11.8411\n",
            "Epoch 80/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8512 - mean_absolute_percentage_error: 7.8512 - val_loss: 11.8131 - val_mean_absolute_percentage_error: 11.8131\n",
            "Epoch 81/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.8570 - mean_absolute_percentage_error: 7.8570 - val_loss: 11.6824 - val_mean_absolute_percentage_error: 11.6824\n",
            "Epoch 82/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.8412 - mean_absolute_percentage_error: 7.8412 - val_loss: 11.6693 - val_mean_absolute_percentage_error: 11.6693\n",
            "Epoch 83/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7535 - mean_absolute_percentage_error: 7.7535 - val_loss: 11.5731 - val_mean_absolute_percentage_error: 11.5731\n",
            "Epoch 84/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6927 - mean_absolute_percentage_error: 7.6927 - val_loss: 11.7142 - val_mean_absolute_percentage_error: 11.7142\n",
            "Epoch 85/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5663 - mean_absolute_percentage_error: 7.5663 - val_loss: 11.4233 - val_mean_absolute_percentage_error: 11.4233\n",
            "Epoch 86/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6874 - mean_absolute_percentage_error: 7.6874 - val_loss: 11.6416 - val_mean_absolute_percentage_error: 11.6416\n",
            "Epoch 87/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6339 - mean_absolute_percentage_error: 7.6339 - val_loss: 11.4953 - val_mean_absolute_percentage_error: 11.4953\n",
            "Epoch 88/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5504 - mean_absolute_percentage_error: 7.5504 - val_loss: 11.8264 - val_mean_absolute_percentage_error: 11.8264\n",
            "Epoch 89/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.6192 - mean_absolute_percentage_error: 7.6192 - val_loss: 11.8484 - val_mean_absolute_percentage_error: 11.8484\n",
            "Epoch 90/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5775 - mean_absolute_percentage_error: 7.5775 - val_loss: 11.4192 - val_mean_absolute_percentage_error: 11.4192\n",
            "Epoch 91/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.8190 - mean_absolute_percentage_error: 7.8190 - val_loss: 12.1814 - val_mean_absolute_percentage_error: 12.1814\n",
            "Epoch 92/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6924 - mean_absolute_percentage_error: 7.6924 - val_loss: 11.5913 - val_mean_absolute_percentage_error: 11.5913\n",
            "Epoch 93/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.7394 - mean_absolute_percentage_error: 7.7394 - val_loss: 11.5717 - val_mean_absolute_percentage_error: 11.5717\n",
            "Epoch 94/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5740 - mean_absolute_percentage_error: 7.5740 - val_loss: 11.0951 - val_mean_absolute_percentage_error: 11.0951\n",
            "Epoch 95/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6877 - mean_absolute_percentage_error: 7.6877 - val_loss: 11.6806 - val_mean_absolute_percentage_error: 11.6806\n",
            "Epoch 96/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.7597 - mean_absolute_percentage_error: 7.7597 - val_loss: 11.5429 - val_mean_absolute_percentage_error: 11.5429\n",
            "Epoch 97/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.6140 - mean_absolute_percentage_error: 7.6140 - val_loss: 11.3337 - val_mean_absolute_percentage_error: 11.3337\n",
            "Epoch 98/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3587 - mean_absolute_percentage_error: 7.3587 - val_loss: 11.3960 - val_mean_absolute_percentage_error: 11.3960\n",
            "Epoch 99/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.5537 - mean_absolute_percentage_error: 7.5537 - val_loss: 11.4290 - val_mean_absolute_percentage_error: 11.4290\n",
            "Epoch 100/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.4712 - mean_absolute_percentage_error: 7.4712 - val_loss: 11.1985 - val_mean_absolute_percentage_error: 11.1985\n",
            "Epoch 101/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1722 - mean_absolute_percentage_error: 7.1722 - val_loss: 11.5294 - val_mean_absolute_percentage_error: 11.5294\n",
            "Epoch 102/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2272 - mean_absolute_percentage_error: 7.2272 - val_loss: 11.4753 - val_mean_absolute_percentage_error: 11.4753\n",
            "Epoch 103/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1251 - mean_absolute_percentage_error: 7.1251 - val_loss: 11.0934 - val_mean_absolute_percentage_error: 11.0934\n",
            "Epoch 104/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.2116 - mean_absolute_percentage_error: 7.2116 - val_loss: 11.3301 - val_mean_absolute_percentage_error: 11.3301\n",
            "Epoch 105/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2219 - mean_absolute_percentage_error: 7.2219 - val_loss: 11.4342 - val_mean_absolute_percentage_error: 11.4342\n",
            "Epoch 106/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1570 - mean_absolute_percentage_error: 7.1570 - val_loss: 11.2831 - val_mean_absolute_percentage_error: 11.2831\n",
            "Epoch 107/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1236 - mean_absolute_percentage_error: 7.1236 - val_loss: 11.3654 - val_mean_absolute_percentage_error: 11.3654\n",
            "Epoch 108/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.3383 - mean_absolute_percentage_error: 7.3383 - val_loss: 11.1060 - val_mean_absolute_percentage_error: 11.1060\n",
            "Epoch 109/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.4250 - mean_absolute_percentage_error: 7.4250 - val_loss: 11.5031 - val_mean_absolute_percentage_error: 11.5031\n",
            "Epoch 110/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1580 - mean_absolute_percentage_error: 7.1580 - val_loss: 11.3032 - val_mean_absolute_percentage_error: 11.3032\n",
            "Epoch 111/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1562 - mean_absolute_percentage_error: 7.1562 - val_loss: 11.3062 - val_mean_absolute_percentage_error: 11.3062\n",
            "Epoch 112/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0689 - mean_absolute_percentage_error: 7.0689 - val_loss: 11.3365 - val_mean_absolute_percentage_error: 11.3365\n",
            "Epoch 113/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.0314 - mean_absolute_percentage_error: 7.0314 - val_loss: 11.3305 - val_mean_absolute_percentage_error: 11.3305\n",
            "Epoch 114/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1234 - mean_absolute_percentage_error: 7.1234 - val_loss: 11.2320 - val_mean_absolute_percentage_error: 11.2320\n",
            "Epoch 115/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0016 - mean_absolute_percentage_error: 7.0016 - val_loss: 11.2497 - val_mean_absolute_percentage_error: 11.2497\n",
            "Epoch 116/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1733 - mean_absolute_percentage_error: 7.1733 - val_loss: 11.4386 - val_mean_absolute_percentage_error: 11.4386\n",
            "Epoch 117/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0940 - mean_absolute_percentage_error: 7.0940 - val_loss: 11.5389 - val_mean_absolute_percentage_error: 11.5389\n",
            "Epoch 118/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.2571 - mean_absolute_percentage_error: 7.2571 - val_loss: 11.5660 - val_mean_absolute_percentage_error: 11.5660\n",
            "Epoch 119/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 7.1957 - mean_absolute_percentage_error: 7.1957 - val_loss: 11.4984 - val_mean_absolute_percentage_error: 11.4984\n",
            "Epoch 120/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0635 - mean_absolute_percentage_error: 7.0635 - val_loss: 11.2643 - val_mean_absolute_percentage_error: 11.2643\n",
            "Epoch 121/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9279 - mean_absolute_percentage_error: 6.9279 - val_loss: 11.4776 - val_mean_absolute_percentage_error: 11.4776\n",
            "Epoch 122/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8904 - mean_absolute_percentage_error: 6.8904 - val_loss: 11.2581 - val_mean_absolute_percentage_error: 11.2581\n",
            "Epoch 123/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9849 - mean_absolute_percentage_error: 6.9849 - val_loss: 11.1620 - val_mean_absolute_percentage_error: 11.1620\n",
            "Epoch 124/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8989 - mean_absolute_percentage_error: 6.8989 - val_loss: 11.2426 - val_mean_absolute_percentage_error: 11.2426\n",
            "Epoch 125/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9136 - mean_absolute_percentage_error: 6.9136 - val_loss: 11.3972 - val_mean_absolute_percentage_error: 11.3972\n",
            "Epoch 126/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7626 - mean_absolute_percentage_error: 6.7626 - val_loss: 10.9649 - val_mean_absolute_percentage_error: 10.9649\n",
            "Epoch 127/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1224 - mean_absolute_percentage_error: 7.1224 - val_loss: 11.0711 - val_mean_absolute_percentage_error: 11.0711\n",
            "Epoch 128/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8421 - mean_absolute_percentage_error: 6.8421 - val_loss: 11.6738 - val_mean_absolute_percentage_error: 11.6738\n",
            "Epoch 129/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.1413 - mean_absolute_percentage_error: 7.1413 - val_loss: 11.0649 - val_mean_absolute_percentage_error: 11.0649\n",
            "Epoch 130/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 7.0878 - mean_absolute_percentage_error: 7.0878 - val_loss: 11.3191 - val_mean_absolute_percentage_error: 11.3191\n",
            "Epoch 131/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9295 - mean_absolute_percentage_error: 6.9295 - val_loss: 11.4327 - val_mean_absolute_percentage_error: 11.4327\n",
            "Epoch 132/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.9328 - mean_absolute_percentage_error: 6.9328 - val_loss: 11.5628 - val_mean_absolute_percentage_error: 11.5628\n",
            "Epoch 133/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7870 - mean_absolute_percentage_error: 6.7870 - val_loss: 11.4923 - val_mean_absolute_percentage_error: 11.4923\n",
            "Epoch 134/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8520 - mean_absolute_percentage_error: 6.8520 - val_loss: 11.2201 - val_mean_absolute_percentage_error: 11.2201\n",
            "Epoch 135/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.9646 - mean_absolute_percentage_error: 6.9646 - val_loss: 11.3260 - val_mean_absolute_percentage_error: 11.3260\n",
            "Epoch 136/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7916 - mean_absolute_percentage_error: 6.7916 - val_loss: 11.8259 - val_mean_absolute_percentage_error: 11.8259\n",
            "Epoch 137/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.1423 - mean_absolute_percentage_error: 7.1423 - val_loss: 11.1751 - val_mean_absolute_percentage_error: 11.1751\n",
            "Epoch 138/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 7.0768 - mean_absolute_percentage_error: 7.0768 - val_loss: 11.6147 - val_mean_absolute_percentage_error: 11.6147\n",
            "Epoch 139/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7235 - mean_absolute_percentage_error: 6.7235 - val_loss: 11.4220 - val_mean_absolute_percentage_error: 11.4220\n",
            "Epoch 140/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6937 - mean_absolute_percentage_error: 6.6937 - val_loss: 11.3638 - val_mean_absolute_percentage_error: 11.3638\n",
            "Epoch 141/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.8319 - mean_absolute_percentage_error: 6.8319 - val_loss: 11.0753 - val_mean_absolute_percentage_error: 11.0753\n",
            "Epoch 142/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8366 - mean_absolute_percentage_error: 6.8366 - val_loss: 11.1994 - val_mean_absolute_percentage_error: 11.1994\n",
            "Epoch 143/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7625 - mean_absolute_percentage_error: 6.7625 - val_loss: 11.3753 - val_mean_absolute_percentage_error: 11.3753\n",
            "Epoch 144/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6017 - mean_absolute_percentage_error: 6.6017 - val_loss: 11.3441 - val_mean_absolute_percentage_error: 11.3441\n",
            "Epoch 145/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6315 - mean_absolute_percentage_error: 6.6315 - val_loss: 11.4508 - val_mean_absolute_percentage_error: 11.4508\n",
            "Epoch 146/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5858 - mean_absolute_percentage_error: 6.5858 - val_loss: 11.3096 - val_mean_absolute_percentage_error: 11.3096\n",
            "Epoch 147/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7032 - mean_absolute_percentage_error: 6.7032 - val_loss: 11.1503 - val_mean_absolute_percentage_error: 11.1503\n",
            "Epoch 148/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6932 - mean_absolute_percentage_error: 6.6932 - val_loss: 11.1569 - val_mean_absolute_percentage_error: 11.1569\n",
            "Epoch 149/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.8797 - mean_absolute_percentage_error: 6.8797 - val_loss: 11.2252 - val_mean_absolute_percentage_error: 11.2252\n",
            "Epoch 150/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5730 - mean_absolute_percentage_error: 6.5730 - val_loss: 10.9945 - val_mean_absolute_percentage_error: 10.9945\n",
            "Epoch 151/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3970 - mean_absolute_percentage_error: 6.3970 - val_loss: 11.2035 - val_mean_absolute_percentage_error: 11.2035\n",
            "Epoch 152/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7462 - mean_absolute_percentage_error: 6.7462 - val_loss: 11.4180 - val_mean_absolute_percentage_error: 11.4180\n",
            "Epoch 153/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5729 - mean_absolute_percentage_error: 6.5729 - val_loss: 11.2513 - val_mean_absolute_percentage_error: 11.2513\n",
            "Epoch 154/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6288 - mean_absolute_percentage_error: 6.6288 - val_loss: 11.2601 - val_mean_absolute_percentage_error: 11.2601\n",
            "Epoch 155/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6116 - mean_absolute_percentage_error: 6.6116 - val_loss: 11.3987 - val_mean_absolute_percentage_error: 11.3987\n",
            "Epoch 156/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.7582 - mean_absolute_percentage_error: 6.7582 - val_loss: 11.0348 - val_mean_absolute_percentage_error: 11.0348\n",
            "Epoch 157/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.8779 - mean_absolute_percentage_error: 6.8779 - val_loss: 11.0484 - val_mean_absolute_percentage_error: 11.0484\n",
            "Epoch 158/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5947 - mean_absolute_percentage_error: 6.5947 - val_loss: 10.9153 - val_mean_absolute_percentage_error: 10.9153\n",
            "Epoch 159/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5943 - mean_absolute_percentage_error: 6.5943 - val_loss: 11.3071 - val_mean_absolute_percentage_error: 11.3071\n",
            "Epoch 160/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5651 - mean_absolute_percentage_error: 6.5651 - val_loss: 11.2231 - val_mean_absolute_percentage_error: 11.2231\n",
            "Epoch 161/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5180 - mean_absolute_percentage_error: 6.5180 - val_loss: 10.8422 - val_mean_absolute_percentage_error: 10.8422\n",
            "Epoch 162/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7895 - mean_absolute_percentage_error: 6.7895 - val_loss: 11.4672 - val_mean_absolute_percentage_error: 11.4672\n",
            "Epoch 163/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.6726 - mean_absolute_percentage_error: 6.6726 - val_loss: 11.2276 - val_mean_absolute_percentage_error: 11.2276\n",
            "Epoch 164/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4420 - mean_absolute_percentage_error: 6.4420 - val_loss: 11.2494 - val_mean_absolute_percentage_error: 11.2494\n",
            "Epoch 165/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4767 - mean_absolute_percentage_error: 6.4767 - val_loss: 11.3248 - val_mean_absolute_percentage_error: 11.3248\n",
            "Epoch 166/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5482 - mean_absolute_percentage_error: 6.5482 - val_loss: 10.9210 - val_mean_absolute_percentage_error: 10.9210\n",
            "Epoch 167/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3299 - mean_absolute_percentage_error: 6.3299 - val_loss: 11.1304 - val_mean_absolute_percentage_error: 11.1304\n",
            "Epoch 168/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2608 - mean_absolute_percentage_error: 6.2608 - val_loss: 11.0045 - val_mean_absolute_percentage_error: 11.0045\n",
            "Epoch 169/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3858 - mean_absolute_percentage_error: 6.3858 - val_loss: 10.9695 - val_mean_absolute_percentage_error: 10.9695\n",
            "Epoch 170/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3508 - mean_absolute_percentage_error: 6.3508 - val_loss: 10.9273 - val_mean_absolute_percentage_error: 10.9273\n",
            "Epoch 171/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3883 - mean_absolute_percentage_error: 6.3883 - val_loss: 10.9099 - val_mean_absolute_percentage_error: 10.9099\n",
            "Epoch 172/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3895 - mean_absolute_percentage_error: 6.3895 - val_loss: 11.6600 - val_mean_absolute_percentage_error: 11.6600\n",
            "Epoch 173/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.5914 - mean_absolute_percentage_error: 6.5914 - val_loss: 11.0433 - val_mean_absolute_percentage_error: 11.0433\n",
            "Epoch 174/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2773 - mean_absolute_percentage_error: 6.2773 - val_loss: 11.1907 - val_mean_absolute_percentage_error: 11.1907\n",
            "Epoch 175/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5490 - mean_absolute_percentage_error: 6.5490 - val_loss: 11.3059 - val_mean_absolute_percentage_error: 11.3059\n",
            "Epoch 176/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.7163 - mean_absolute_percentage_error: 6.7163 - val_loss: 11.3463 - val_mean_absolute_percentage_error: 11.3463\n",
            "Epoch 177/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3136 - mean_absolute_percentage_error: 6.3136 - val_loss: 11.4509 - val_mean_absolute_percentage_error: 11.4509\n",
            "Epoch 178/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3713 - mean_absolute_percentage_error: 6.3713 - val_loss: 11.0258 - val_mean_absolute_percentage_error: 11.0258\n",
            "Epoch 179/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3416 - mean_absolute_percentage_error: 6.3416 - val_loss: 10.7453 - val_mean_absolute_percentage_error: 10.7453\n",
            "Epoch 180/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3372 - mean_absolute_percentage_error: 6.3372 - val_loss: 11.1227 - val_mean_absolute_percentage_error: 11.1227\n",
            "Epoch 181/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2055 - mean_absolute_percentage_error: 6.2055 - val_loss: 11.1516 - val_mean_absolute_percentage_error: 11.1516\n",
            "Epoch 182/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2906 - mean_absolute_percentage_error: 6.2906 - val_loss: 11.1591 - val_mean_absolute_percentage_error: 11.1591\n",
            "Epoch 183/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2187 - mean_absolute_percentage_error: 6.2187 - val_loss: 11.0606 - val_mean_absolute_percentage_error: 11.0606\n",
            "Epoch 184/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3168 - mean_absolute_percentage_error: 6.3168 - val_loss: 11.0992 - val_mean_absolute_percentage_error: 11.0992\n",
            "Epoch 185/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2332 - mean_absolute_percentage_error: 6.2332 - val_loss: 11.2232 - val_mean_absolute_percentage_error: 11.2232\n",
            "Epoch 186/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1234 - mean_absolute_percentage_error: 6.1234 - val_loss: 11.2149 - val_mean_absolute_percentage_error: 11.2149\n",
            "Epoch 187/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3087 - mean_absolute_percentage_error: 6.3087 - val_loss: 11.4536 - val_mean_absolute_percentage_error: 11.4536\n",
            "Epoch 188/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.3455 - mean_absolute_percentage_error: 6.3455 - val_loss: 10.9443 - val_mean_absolute_percentage_error: 10.9443\n",
            "Epoch 189/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3329 - mean_absolute_percentage_error: 6.3329 - val_loss: 11.0582 - val_mean_absolute_percentage_error: 11.0582\n",
            "Epoch 190/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.1459 - mean_absolute_percentage_error: 6.1459 - val_loss: 11.1489 - val_mean_absolute_percentage_error: 11.1489\n",
            "Epoch 191/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1810 - mean_absolute_percentage_error: 6.1810 - val_loss: 11.0393 - val_mean_absolute_percentage_error: 11.0393\n",
            "Epoch 192/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4176 - mean_absolute_percentage_error: 6.4176 - val_loss: 10.6249 - val_mean_absolute_percentage_error: 10.6249\n",
            "Epoch 193/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2808 - mean_absolute_percentage_error: 6.2808 - val_loss: 11.2805 - val_mean_absolute_percentage_error: 11.2805\n",
            "Epoch 194/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.2928 - mean_absolute_percentage_error: 6.2928 - val_loss: 11.3116 - val_mean_absolute_percentage_error: 11.3116\n",
            "Epoch 195/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5418 - mean_absolute_percentage_error: 6.5418 - val_loss: 11.2769 - val_mean_absolute_percentage_error: 11.2769\n",
            "Epoch 196/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.6050 - mean_absolute_percentage_error: 6.6050 - val_loss: 10.7880 - val_mean_absolute_percentage_error: 10.7880\n",
            "Epoch 197/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.5486 - mean_absolute_percentage_error: 6.5486 - val_loss: 11.1742 - val_mean_absolute_percentage_error: 11.1742\n",
            "Epoch 198/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3289 - mean_absolute_percentage_error: 6.3289 - val_loss: 11.0663 - val_mean_absolute_percentage_error: 11.0663\n",
            "Epoch 199/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3304 - mean_absolute_percentage_error: 6.3304 - val_loss: 11.1869 - val_mean_absolute_percentage_error: 11.1869\n",
            "Epoch 200/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.1514 - mean_absolute_percentage_error: 6.1514 - val_loss: 11.0133 - val_mean_absolute_percentage_error: 11.0133\n",
            "Epoch 201/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1063 - mean_absolute_percentage_error: 6.1063 - val_loss: 11.2646 - val_mean_absolute_percentage_error: 11.2646\n",
            "Epoch 202/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0764 - mean_absolute_percentage_error: 6.0764 - val_loss: 10.9810 - val_mean_absolute_percentage_error: 10.9810\n",
            "Epoch 203/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9301 - mean_absolute_percentage_error: 5.9301 - val_loss: 11.5127 - val_mean_absolute_percentage_error: 11.5127\n",
            "Epoch 204/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2525 - mean_absolute_percentage_error: 6.2525 - val_loss: 10.8079 - val_mean_absolute_percentage_error: 10.8079\n",
            "Epoch 205/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3399 - mean_absolute_percentage_error: 6.3399 - val_loss: 11.4999 - val_mean_absolute_percentage_error: 11.4999\n",
            "Epoch 206/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2945 - mean_absolute_percentage_error: 6.2945 - val_loss: 11.3372 - val_mean_absolute_percentage_error: 11.3372\n",
            "Epoch 207/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.3994 - mean_absolute_percentage_error: 6.3994 - val_loss: 10.5509 - val_mean_absolute_percentage_error: 10.5509\n",
            "Epoch 208/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4352 - mean_absolute_percentage_error: 6.4352 - val_loss: 11.0900 - val_mean_absolute_percentage_error: 11.0900\n",
            "Epoch 209/250\n",
            "12/12 [==============================] - 0s 7ms/step - loss: 6.0442 - mean_absolute_percentage_error: 6.0442 - val_loss: 11.2952 - val_mean_absolute_percentage_error: 11.2952\n",
            "Epoch 210/250\n",
            "12/12 [==============================] - 0s 6ms/step - loss: 6.3316 - mean_absolute_percentage_error: 6.3316 - val_loss: 10.9328 - val_mean_absolute_percentage_error: 10.9328\n",
            "Epoch 211/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9679 - mean_absolute_percentage_error: 5.9679 - val_loss: 10.9743 - val_mean_absolute_percentage_error: 10.9743\n",
            "Epoch 212/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0088 - mean_absolute_percentage_error: 6.0088 - val_loss: 11.2341 - val_mean_absolute_percentage_error: 11.2341\n",
            "Epoch 213/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4633 - mean_absolute_percentage_error: 6.4633 - val_loss: 11.4069 - val_mean_absolute_percentage_error: 11.4069\n",
            "Epoch 214/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1415 - mean_absolute_percentage_error: 6.1415 - val_loss: 10.8961 - val_mean_absolute_percentage_error: 10.8961\n",
            "Epoch 215/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0220 - mean_absolute_percentage_error: 6.0220 - val_loss: 10.9206 - val_mean_absolute_percentage_error: 10.9206\n",
            "Epoch 216/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0985 - mean_absolute_percentage_error: 6.0985 - val_loss: 10.8869 - val_mean_absolute_percentage_error: 10.8869\n",
            "Epoch 217/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1317 - mean_absolute_percentage_error: 6.1317 - val_loss: 11.0093 - val_mean_absolute_percentage_error: 11.0093\n",
            "Epoch 218/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.4207 - mean_absolute_percentage_error: 6.4207 - val_loss: 11.2881 - val_mean_absolute_percentage_error: 11.2881\n",
            "Epoch 219/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9157 - mean_absolute_percentage_error: 5.9157 - val_loss: 10.9124 - val_mean_absolute_percentage_error: 10.9124\n",
            "Epoch 220/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1985 - mean_absolute_percentage_error: 6.1985 - val_loss: 10.8430 - val_mean_absolute_percentage_error: 10.8430\n",
            "Epoch 221/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1000 - mean_absolute_percentage_error: 6.1000 - val_loss: 11.3591 - val_mean_absolute_percentage_error: 11.3591\n",
            "Epoch 222/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1582 - mean_absolute_percentage_error: 6.1582 - val_loss: 11.2969 - val_mean_absolute_percentage_error: 11.2969\n",
            "Epoch 223/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.2470 - mean_absolute_percentage_error: 6.2470 - val_loss: 10.8864 - val_mean_absolute_percentage_error: 10.8864\n",
            "Epoch 224/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1824 - mean_absolute_percentage_error: 6.1824 - val_loss: 10.9033 - val_mean_absolute_percentage_error: 10.9033\n",
            "Epoch 225/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1892 - mean_absolute_percentage_error: 6.1892 - val_loss: 11.0047 - val_mean_absolute_percentage_error: 11.0047\n",
            "Epoch 226/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0535 - mean_absolute_percentage_error: 6.0535 - val_loss: 10.6964 - val_mean_absolute_percentage_error: 10.6964\n",
            "Epoch 227/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0028 - mean_absolute_percentage_error: 6.0028 - val_loss: 10.9384 - val_mean_absolute_percentage_error: 10.9384\n",
            "Epoch 228/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8427 - mean_absolute_percentage_error: 5.8427 - val_loss: 11.2807 - val_mean_absolute_percentage_error: 11.2807\n",
            "Epoch 229/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8849 - mean_absolute_percentage_error: 5.8849 - val_loss: 10.8249 - val_mean_absolute_percentage_error: 10.8249\n",
            "Epoch 230/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9162 - mean_absolute_percentage_error: 5.9162 - val_loss: 11.1615 - val_mean_absolute_percentage_error: 11.1615\n",
            "Epoch 231/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9491 - mean_absolute_percentage_error: 5.9491 - val_loss: 10.9755 - val_mean_absolute_percentage_error: 10.9755\n",
            "Epoch 232/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 5.8375 - mean_absolute_percentage_error: 5.8375 - val_loss: 10.9548 - val_mean_absolute_percentage_error: 10.9548\n",
            "Epoch 233/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8351 - mean_absolute_percentage_error: 5.8351 - val_loss: 10.8491 - val_mean_absolute_percentage_error: 10.8491\n",
            "Epoch 234/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9789 - mean_absolute_percentage_error: 5.9789 - val_loss: 11.0601 - val_mean_absolute_percentage_error: 11.0601\n",
            "Epoch 235/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7075 - mean_absolute_percentage_error: 5.7075 - val_loss: 10.8839 - val_mean_absolute_percentage_error: 10.8839\n",
            "Epoch 236/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8845 - mean_absolute_percentage_error: 5.8845 - val_loss: 10.7246 - val_mean_absolute_percentage_error: 10.7246\n",
            "Epoch 237/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9612 - mean_absolute_percentage_error: 5.9612 - val_loss: 11.0609 - val_mean_absolute_percentage_error: 11.0609\n",
            "Epoch 238/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.1374 - mean_absolute_percentage_error: 6.1374 - val_loss: 11.3242 - val_mean_absolute_percentage_error: 11.3242\n",
            "Epoch 239/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9777 - mean_absolute_percentage_error: 5.9777 - val_loss: 10.8385 - val_mean_absolute_percentage_error: 10.8385\n",
            "Epoch 240/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8946 - mean_absolute_percentage_error: 5.8946 - val_loss: 10.7831 - val_mean_absolute_percentage_error: 10.7831\n",
            "Epoch 241/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9827 - mean_absolute_percentage_error: 5.9827 - val_loss: 10.7506 - val_mean_absolute_percentage_error: 10.7506\n",
            "Epoch 242/250\n",
            "12/12 [==============================] - 0s 4ms/step - loss: 6.0594 - mean_absolute_percentage_error: 6.0594 - val_loss: 11.9924 - val_mean_absolute_percentage_error: 11.9924\n",
            "Epoch 243/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 6.0982 - mean_absolute_percentage_error: 6.0982 - val_loss: 10.7078 - val_mean_absolute_percentage_error: 10.7078\n",
            "Epoch 244/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9816 - mean_absolute_percentage_error: 5.9816 - val_loss: 11.0171 - val_mean_absolute_percentage_error: 11.0171\n",
            "Epoch 245/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.7929 - mean_absolute_percentage_error: 5.7929 - val_loss: 11.0052 - val_mean_absolute_percentage_error: 11.0052\n",
            "Epoch 246/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8823 - mean_absolute_percentage_error: 5.8823 - val_loss: 11.3532 - val_mean_absolute_percentage_error: 11.3532\n",
            "Epoch 247/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9538 - mean_absolute_percentage_error: 5.9538 - val_loss: 11.0902 - val_mean_absolute_percentage_error: 11.0902\n",
            "Epoch 248/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8252 - mean_absolute_percentage_error: 5.8252 - val_loss: 10.9742 - val_mean_absolute_percentage_error: 10.9742\n",
            "Epoch 249/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.8779 - mean_absolute_percentage_error: 5.8779 - val_loss: 11.0614 - val_mean_absolute_percentage_error: 11.0614\n",
            "Epoch 250/250\n",
            "12/12 [==============================] - 0s 5ms/step - loss: 5.9546 - mean_absolute_percentage_error: 5.9546 - val_loss: 11.8247 - val_mean_absolute_percentage_error: 11.8247\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0429cb3110>"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mupD9JvzD1BU"
      },
      "source": [
        "#Fun Fact\n",
        "\n",
        "Google Translate is getting better all the time, but it's still not perfect. Translate a sentence into another language and back into English, and you might get a hilarious surprise. That's what Malinda Kathleen Reese got when she reverse Google Translated the lyrics to \"Let It Go\" from Disney's Frozen into Chinese, Macedonian, French, Polish, Creole, Tamil and others. It doesn't come out as utter gibberish, but as a slightly off version with a slightly different message from the original. Which makes it even funnier. Plus, Malinda can really sing.\n",
        "\n",
        "Link to video: https://www.youtube.com/watch?v=2bVAoVlFYf0"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5t0A5Xui7sum"
      },
      "source": [
        "# Classification Losses\n",
        "\n",
        "In classification, the outputs are in form of a class or a category. The label or number assigned to the classes do not have a numerical meaning. \n",
        "\n",
        "For example, an input with class label 0 cannot be numerically compared with an input with class label 1. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ERGOjkbwjCT1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3338356-7834-4cfa-d98e-a298b4100033"
      },
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "169009152/169001437 [==============================] - 2s 0us/step\n",
            "169017344/169001437 [==============================] - 2s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9EbtSlPYZYju"
      },
      "source": [
        "## Kullback-Leibler Divergence [KDL]\n",
        "\n",
        "Kullback Leibler Divergence Loss is a measure of how a distribution varies from a reference distribution (or a baseline distribution). A Kullback Leibler Divergence Loss of zero means that both the probability distributions are identical.\n",
        "\n",
        "The number of information lost in the predicted distribution is used as a measure.\n",
        "\n",
        "$$KDL(p||q) = \\int_x p(x) \\log \\frac{p(x)}{q(x)} dx$$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IkyXvdIQZZyO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "174a3341-ecfd-44ea-db37-b016f8d8c59f"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='kl_divergence', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 16s 5ms/step - loss: 437.4926 - accuracy: 0.0208 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4918 - accuracy: 0.0273 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0206 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4926 - accuracy: 0.0220 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4921 - accuracy: 0.0164 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4918 - accuracy: 0.0112 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0098 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4928 - accuracy: 0.0125 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0117 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0086 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4921 - accuracy: 0.0091 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4922 - accuracy: 0.0114 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0131 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0100 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0096 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4924 - accuracy: 0.0088 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0104 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4919 - accuracy: 0.0094 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4920 - accuracy: 0.0085 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4921 - accuracy: 0.0091 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0118 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4918 - accuracy: 0.0099 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0107 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4922 - accuracy: 0.0114 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0084 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4916 - accuracy: 0.0083 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4919 - accuracy: 0.0085 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4922 - accuracy: 0.0107 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4924 - accuracy: 0.0101 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4926 - accuracy: 0.0103 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4919 - accuracy: 0.0104 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4925 - accuracy: 0.0129 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4924 - accuracy: 0.0092 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4923 - accuracy: 0.0090 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0107 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4923 - accuracy: 0.0090 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 437.4924 - accuracy: 0.0098 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0112 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4925 - accuracy: 0.0082 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4920 - accuracy: 0.0088 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4921 - accuracy: 0.0096 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4923 - accuracy: 0.0098 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4921 - accuracy: 0.0105 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0100 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0131 - val_loss: 437.4907 - val_accuracy: 0.0500\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4921 - accuracy: 0.0111 - val_loss: 437.4908 - val_accuracy: 0.0500\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0092 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4922 - accuracy: 0.0089 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 437.4920 - accuracy: 0.0095 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 437.4923 - accuracy: 0.0101 - val_loss: 437.4907 - val_accuracy: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0429e74f50>"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AIOrqUGTYjD8"
      },
      "source": [
        "##Binary Cross Entropy\n",
        "Cross-entropy is a measure from the field of information theory, building upon entropy and generally calculating the difference between two probability distributions. It is closely related to but is different from KL divergence that calculates the relative entropy between two probability distributions, whereas cross-entropy can be thought to calculate the total entropy between the distributions.\n",
        "\n",
        "Cross-entropy is also related to and often confused with logistic loss, called log loss. Although the two measures are derived from a different source, when used as loss functions for classification models, both measures calculate the same quantity and can be used interchangeably.\n",
        "\n",
        "Binary crossentropy is a loss function that is used in binary classification tasks. These are tasks that answer a question with only two choices (yes or no, A or B, 0 or 1, left or right). Several independent such questions can be answered at the same time, as in multi-label classification or in binary image segmentation. Formally, this loss is equal to the average of the categorical crossentropy loss on many two-category tasks.\n",
        "\n",
        "$$BCE = -\\frac{1}{N} \\sum_{i=1}^N y_i \\cdot \\log(p(y_i)) + (1-y_i) \\cdot \\log(1- p(y_i))$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_labels = tf.reshape(tf.one_hot(training_labels, 100), [training_labels.shape[0], 100])\n",
        "print(training_labels.shape)\n",
        "\n",
        "test_labels = tf.reshape(tf.one_hot(test_labels, 100), [test_labels.shape[0], 100])\n",
        "print(test_labels.shape)"
      ],
      "metadata": {
        "id": "Ckc0jYu_LwkP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd81354f-c2e0-4f9a-b0e9-7d52743b666d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 100)\n",
            "(10000, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NRvhNnnJYjOF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "824fd904-e565-45ff-ad1c-4dbbe90ecca8"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0429 - accuracy: 0.1457 - val_loss: 0.0349 - val_accuracy: 0.2215\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0333 - accuracy: 0.2619 - val_loss: 0.0321 - val_accuracy: 0.2961\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0314 - accuracy: 0.3132 - val_loss: 0.0307 - val_accuracy: 0.3229\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0300 - accuracy: 0.3483 - val_loss: 0.0301 - val_accuracy: 0.3420\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0290 - accuracy: 0.3731 - val_loss: 0.0291 - val_accuracy: 0.3699\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0281 - accuracy: 0.3976 - val_loss: 0.0282 - val_accuracy: 0.3894\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0274 - accuracy: 0.4131 - val_loss: 0.0282 - val_accuracy: 0.3879\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0268 - accuracy: 0.4273 - val_loss: 0.0272 - val_accuracy: 0.4148\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0263 - accuracy: 0.4405 - val_loss: 0.0274 - val_accuracy: 0.4152\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0258 - accuracy: 0.4538 - val_loss: 0.0269 - val_accuracy: 0.4238\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0254 - accuracy: 0.4619 - val_loss: 0.0260 - val_accuracy: 0.4544\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0250 - accuracy: 0.4717 - val_loss: 0.0264 - val_accuracy: 0.4419\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0247 - accuracy: 0.4800 - val_loss: 0.0266 - val_accuracy: 0.4355\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0244 - accuracy: 0.4876 - val_loss: 0.0262 - val_accuracy: 0.4481\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0241 - accuracy: 0.4923 - val_loss: 0.0261 - val_accuracy: 0.4461\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0239 - accuracy: 0.4997 - val_loss: 0.0261 - val_accuracy: 0.4430\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0236 - accuracy: 0.5065 - val_loss: 0.0259 - val_accuracy: 0.4558\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0234 - accuracy: 0.5121 - val_loss: 0.0260 - val_accuracy: 0.4563\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0232 - accuracy: 0.5156 - val_loss: 0.0255 - val_accuracy: 0.4615\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0230 - accuracy: 0.5188 - val_loss: 0.0253 - val_accuracy: 0.4646\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0228 - accuracy: 0.5259 - val_loss: 0.0253 - val_accuracy: 0.4701\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0227 - accuracy: 0.5263 - val_loss: 0.0250 - val_accuracy: 0.4777\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0225 - accuracy: 0.5331 - val_loss: 0.0250 - val_accuracy: 0.4763\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0224 - accuracy: 0.5366 - val_loss: 0.0249 - val_accuracy: 0.4789\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0222 - accuracy: 0.5385 - val_loss: 0.0247 - val_accuracy: 0.4779\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0221 - accuracy: 0.5404 - val_loss: 0.0248 - val_accuracy: 0.4790\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0220 - accuracy: 0.5456 - val_loss: 0.0251 - val_accuracy: 0.4727\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0219 - accuracy: 0.5476 - val_loss: 0.0264 - val_accuracy: 0.4487\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0217 - accuracy: 0.5491 - val_loss: 0.0246 - val_accuracy: 0.4873\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0216 - accuracy: 0.5523 - val_loss: 0.0253 - val_accuracy: 0.4730\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0216 - accuracy: 0.5519 - val_loss: 0.0248 - val_accuracy: 0.4820\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0215 - accuracy: 0.5560 - val_loss: 0.0251 - val_accuracy: 0.4802\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0214 - accuracy: 0.5585 - val_loss: 0.0257 - val_accuracy: 0.4657\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0213 - accuracy: 0.5598 - val_loss: 0.0252 - val_accuracy: 0.4805\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0212 - accuracy: 0.5641 - val_loss: 0.0248 - val_accuracy: 0.4814\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0211 - accuracy: 0.5643 - val_loss: 0.0251 - val_accuracy: 0.4813\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0210 - accuracy: 0.5674 - val_loss: 0.0248 - val_accuracy: 0.4890\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0210 - accuracy: 0.5668 - val_loss: 0.0253 - val_accuracy: 0.4750\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0209 - accuracy: 0.5695 - val_loss: 0.0250 - val_accuracy: 0.4878\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0208 - accuracy: 0.5727 - val_loss: 0.0250 - val_accuracy: 0.4847\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0208 - accuracy: 0.5737 - val_loss: 0.0251 - val_accuracy: 0.4938\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0207 - accuracy: 0.5753 - val_loss: 0.0254 - val_accuracy: 0.4815\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0207 - accuracy: 0.5751 - val_loss: 0.0250 - val_accuracy: 0.4902\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0205 - accuracy: 0.5810 - val_loss: 0.0249 - val_accuracy: 0.4850\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0205 - accuracy: 0.5798 - val_loss: 0.0255 - val_accuracy: 0.4752\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0204 - accuracy: 0.5813 - val_loss: 0.0254 - val_accuracy: 0.4842\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0204 - accuracy: 0.5829 - val_loss: 0.0253 - val_accuracy: 0.4827\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0203 - accuracy: 0.5838 - val_loss: 0.0251 - val_accuracy: 0.4919\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.0202 - accuracy: 0.5854 - val_loss: 0.0256 - val_accuracy: 0.4782\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.0202 - accuracy: 0.5870 - val_loss: 0.0251 - val_accuracy: 0.4847\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f041939be10>"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 7\n",
        "\n",
        "Do you see any problems/errors with the above code? Please describe.\n",
        "\n",
        "< The loss value is quite low for all epochs. Additionally, as time passes, it seems that the loss value is getting closer and closer to 0.02, but never reaching the value (it levels off / might have an asymptote at 0.02). This may be an error. >"
      ],
      "metadata": {
        "id": "eAAuecZfTfx2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDBdn-P976v9"
      },
      "source": [
        "## Categorical Cross Entropy\n",
        "\n",
        "This is the most common setting for classification problems. Cross-entropy loss increases as the **predicted probability** strays away from the **actual label**.\n",
        "\n",
        "Note that we have to compare the probabilities (e.g. [0.20, 0.75, 0.05]) of all the classes with the actual labels (e.g., [0, 1, 0]). The actual labels would be one-hot encoding.\n",
        "\n",
        "An important aspect of this is that cross entropy loss penalizes heavily the predictions that are confident but wrong.\n",
        "\n",
        "We are multiplying the log of the actual predicted probability for the ground truth class.\n",
        "\n",
        "$$CCE = -\\frac{1}{N}\\sum_{i=1}^{N}y_i\\log(\\hat{y}_i)$$ "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uskCvsUSrR0o",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e1b214da-3619-48ef-aaf4-d38d2be4eaba"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=25, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.6038 - accuracy: 0.1955 - val_loss: 2.3354 - val_accuracy: 0.2781\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.2502 - accuracy: 0.3007 - val_loss: 2.2387 - val_accuracy: 0.3021\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.1181 - accuracy: 0.3411 - val_loss: 2.1534 - val_accuracy: 0.3327\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0268 - accuracy: 0.3687 - val_loss: 2.0335 - val_accuracy: 0.3668\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.9625 - accuracy: 0.3891 - val_loss: 1.9725 - val_accuracy: 0.3868\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.9022 - accuracy: 0.4062 - val_loss: 1.9140 - val_accuracy: 0.4034\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.8530 - accuracy: 0.4196 - val_loss: 1.8905 - val_accuracy: 0.4094\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.8052 - accuracy: 0.4339 - val_loss: 1.8705 - val_accuracy: 0.4145\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7671 - accuracy: 0.4456 - val_loss: 1.8933 - val_accuracy: 0.4080\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7363 - accuracy: 0.4545 - val_loss: 1.8394 - val_accuracy: 0.4223\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.7067 - accuracy: 0.4651 - val_loss: 1.8041 - val_accuracy: 0.4367\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6803 - accuracy: 0.4713 - val_loss: 1.8082 - val_accuracy: 0.4367\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6572 - accuracy: 0.4769 - val_loss: 1.8292 - val_accuracy: 0.4289\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.6324 - accuracy: 0.4846 - val_loss: 1.7894 - val_accuracy: 0.4440\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6082 - accuracy: 0.4901 - val_loss: 1.7792 - val_accuracy: 0.4465\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5864 - accuracy: 0.4991 - val_loss: 1.7787 - val_accuracy: 0.4530\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5736 - accuracy: 0.5016 - val_loss: 1.7560 - val_accuracy: 0.4528\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5525 - accuracy: 0.5074 - val_loss: 1.8170 - val_accuracy: 0.4439\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5399 - accuracy: 0.5118 - val_loss: 1.8046 - val_accuracy: 0.4515\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.5235 - accuracy: 0.5163 - val_loss: 1.7884 - val_accuracy: 0.4465\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5107 - accuracy: 0.5208 - val_loss: 1.7718 - val_accuracy: 0.4603\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5006 - accuracy: 0.5217 - val_loss: 1.7464 - val_accuracy: 0.4613\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4835 - accuracy: 0.5307 - val_loss: 1.7692 - val_accuracy: 0.4671\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4728 - accuracy: 0.5326 - val_loss: 1.7806 - val_accuracy: 0.4561\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4622 - accuracy: 0.5337 - val_loss: 1.7709 - val_accuracy: 0.4612\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0418a331d0>"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Question 8\n",
        "\n",
        "Now that you know how CCE works, you need to code it. It should give the same answer as `tf.keras.metrics.categorical_crossentropy` would."
      ],
      "metadata": {
        "id": "KYuLKOGSR4yS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Answer 8"
      ],
      "metadata": {
        "id": "TAtqr73-SLUt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def categorical_crossentropy(true, pred):\n",
        "    \n",
        "    loss = -1 * np.sum(true * np.log(pred))\n",
        "\n",
        "    return loss\n",
        "\n",
        "true = tf.constant([[0.0, 1.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [1.0, 0.0, 0.0],\n",
        "                    [0.0, 0.0, 1.0]])\n",
        "pred = tf.constant([[0.20, 0.70, 0.10],\n",
        "                    [0.80, 0.05, 0.15],\n",
        "                    [0.75, 0.10, 0.15],\n",
        "                    [0.25, 0.15, 0.60]])\n",
        "\n",
        "loss = categorical_crossentropy(true, pred)\n",
        "print(loss)\n",
        "\n",
        "loss = tf.keras.metrics.categorical_crossentropy(true, pred)\n",
        "loss = tf.reduce_mean(loss)\n",
        "print(loss)"
      ],
      "metadata": {
        "id": "mxzrT-GKSRSb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57c6ab2-2097-4ddc-9fcd-db40b6ed4bf2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1.378326177597046\n",
            "tf.Tensor(0.34458154, shape=(), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5CHuqni18OPL"
      },
      "source": [
        "## Sparse Categorical Cross Entropy\n",
        "\n",
        "Both, Categorical Cross Entropy [CCE] and Sparse Categorical Cross Entropy [SCCE] have the same loss function. The only difference is the format of $y_i$ (i.e., true labels).\n",
        "\n",
        "If $y_i$'s are one-hot encoded, we should use CCE. Examples (for a 3-class classification): [1,0,0], [0,1,0], [0,0,1]\n",
        "\n",
        "But if $y_i$'s are integers, use SCCE. Examples for above 3-class classification problem: [1], [2], [3]\n",
        "\n",
        "The usage entirely depends on how we load our dataset. One advantage of using sparse categorical cross entropy is it saves time in memory as well as computation because it simply uses a single integer for a class, rather than a whole vector.\n",
        "\n",
        "$$SCCE = -\\log(\\hat{y}_i)$$ for $i$ where $one\\text{-}hot\\text{-}encoding[i] = 1$ "
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "(training_images, training_labels), (test_images, test_labels) = tf.keras.datasets.cifar100.load_data(label_mode=\"coarse\")\n",
        "\n",
        "training_images=training_images.reshape(50000, 32, 32, 3)\n",
        "training_images=training_images / 255.0\n",
        "test_images = test_images.reshape(10000, 32, 32, 3)\n",
        "test_images=test_images/255.0"
      ],
      "metadata": {
        "id": "tb6GisE4SkhX"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dcFK26KvCp-g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9532d46d-3683-4bf0-a07e-d5f3782e127b"
      },
      "source": [
        "model = tf.keras.models.Sequential([\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu', input_shape=(32, 32, 3)),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Conv2D(32, (3,3), activation='relu'),\n",
        "  tf.keras.layers.MaxPooling2D(2, 2),\n",
        "  tf.keras.layers.Flatten(),\n",
        "  tf.keras.layers.Dense(128, activation='relu'),\n",
        "  tf.keras.layers.Dense(100, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "model.fit(training_images, training_labels, epochs=50, validation_data=(test_images, test_labels))"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.5529 - accuracy: 0.2138 - val_loss: 2.3349 - val_accuracy: 0.2813\n",
            "Epoch 2/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.2008 - accuracy: 0.3204 - val_loss: 2.1468 - val_accuracy: 0.3320\n",
            "Epoch 3/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0469 - accuracy: 0.3617 - val_loss: 2.0332 - val_accuracy: 0.3692\n",
            "Epoch 4/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.9436 - accuracy: 0.3920 - val_loss: 1.9591 - val_accuracy: 0.3929\n",
            "Epoch 5/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.8714 - accuracy: 0.4167 - val_loss: 1.8816 - val_accuracy: 0.4104\n",
            "Epoch 6/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.8101 - accuracy: 0.4341 - val_loss: 1.8762 - val_accuracy: 0.4149\n",
            "Epoch 7/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7636 - accuracy: 0.4481 - val_loss: 1.8758 - val_accuracy: 0.4159\n",
            "Epoch 8/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.7263 - accuracy: 0.4573 - val_loss: 1.8629 - val_accuracy: 0.4227\n",
            "Epoch 9/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6893 - accuracy: 0.4676 - val_loss: 1.8049 - val_accuracy: 0.4362\n",
            "Epoch 10/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.6542 - accuracy: 0.4778 - val_loss: 1.7689 - val_accuracy: 0.4446\n",
            "Epoch 11/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6274 - accuracy: 0.4884 - val_loss: 1.7665 - val_accuracy: 0.4472\n",
            "Epoch 12/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.6007 - accuracy: 0.4929 - val_loss: 1.7779 - val_accuracy: 0.4478\n",
            "Epoch 13/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5748 - accuracy: 0.5031 - val_loss: 1.7729 - val_accuracy: 0.4466\n",
            "Epoch 14/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.5560 - accuracy: 0.5074 - val_loss: 1.7239 - val_accuracy: 0.4599\n",
            "Epoch 15/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5340 - accuracy: 0.5120 - val_loss: 1.7944 - val_accuracy: 0.4447\n",
            "Epoch 16/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.5194 - accuracy: 0.5181 - val_loss: 1.7361 - val_accuracy: 0.4610\n",
            "Epoch 17/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4982 - accuracy: 0.5232 - val_loss: 1.7606 - val_accuracy: 0.4511\n",
            "Epoch 18/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4815 - accuracy: 0.5279 - val_loss: 1.7637 - val_accuracy: 0.4607\n",
            "Epoch 19/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4678 - accuracy: 0.5316 - val_loss: 1.7503 - val_accuracy: 0.4570\n",
            "Epoch 20/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4528 - accuracy: 0.5359 - val_loss: 1.7100 - val_accuracy: 0.4706\n",
            "Epoch 21/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.4434 - accuracy: 0.5405 - val_loss: 1.7382 - val_accuracy: 0.4634\n",
            "Epoch 22/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4284 - accuracy: 0.5418 - val_loss: 1.7538 - val_accuracy: 0.4627\n",
            "Epoch 23/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4181 - accuracy: 0.5483 - val_loss: 1.7550 - val_accuracy: 0.4631\n",
            "Epoch 24/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.4027 - accuracy: 0.5505 - val_loss: 1.7425 - val_accuracy: 0.4644\n",
            "Epoch 25/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3964 - accuracy: 0.5527 - val_loss: 1.7340 - val_accuracy: 0.4656\n",
            "Epoch 26/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3847 - accuracy: 0.5575 - val_loss: 1.7788 - val_accuracy: 0.4666\n",
            "Epoch 27/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3740 - accuracy: 0.5614 - val_loss: 1.7313 - val_accuracy: 0.4702\n",
            "Epoch 28/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3641 - accuracy: 0.5619 - val_loss: 1.7829 - val_accuracy: 0.4641\n",
            "Epoch 29/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3502 - accuracy: 0.5676 - val_loss: 1.7649 - val_accuracy: 0.4675\n",
            "Epoch 30/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3406 - accuracy: 0.5699 - val_loss: 1.7666 - val_accuracy: 0.4707\n",
            "Epoch 31/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3295 - accuracy: 0.5733 - val_loss: 1.7474 - val_accuracy: 0.4738\n",
            "Epoch 32/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3226 - accuracy: 0.5746 - val_loss: 1.7710 - val_accuracy: 0.4666\n",
            "Epoch 33/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.3114 - accuracy: 0.5775 - val_loss: 1.7749 - val_accuracy: 0.4660\n",
            "Epoch 34/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.3022 - accuracy: 0.5818 - val_loss: 1.8024 - val_accuracy: 0.4590\n",
            "Epoch 35/50\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 1.3008 - accuracy: 0.5836 - val_loss: 1.7714 - val_accuracy: 0.4692\n",
            "Epoch 36/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2876 - accuracy: 0.5848 - val_loss: 1.7985 - val_accuracy: 0.4675\n",
            "Epoch 37/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2801 - accuracy: 0.5865 - val_loss: 1.7784 - val_accuracy: 0.4663\n",
            "Epoch 38/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2715 - accuracy: 0.5904 - val_loss: 1.7709 - val_accuracy: 0.4690\n",
            "Epoch 39/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2658 - accuracy: 0.5899 - val_loss: 1.8069 - val_accuracy: 0.4640\n",
            "Epoch 40/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2559 - accuracy: 0.5939 - val_loss: 1.7955 - val_accuracy: 0.4692\n",
            "Epoch 41/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2490 - accuracy: 0.5959 - val_loss: 1.7762 - val_accuracy: 0.4743\n",
            "Epoch 42/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2441 - accuracy: 0.5983 - val_loss: 1.7704 - val_accuracy: 0.4704\n",
            "Epoch 43/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2361 - accuracy: 0.6011 - val_loss: 1.8048 - val_accuracy: 0.4671\n",
            "Epoch 44/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2299 - accuracy: 0.6024 - val_loss: 1.8324 - val_accuracy: 0.4706\n",
            "Epoch 45/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.2224 - accuracy: 0.6054 - val_loss: 1.8102 - val_accuracy: 0.4689\n",
            "Epoch 46/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2155 - accuracy: 0.6069 - val_loss: 1.8458 - val_accuracy: 0.4719\n",
            "Epoch 47/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2100 - accuracy: 0.6089 - val_loss: 1.9004 - val_accuracy: 0.4525\n",
            "Epoch 48/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2033 - accuracy: 0.6083 - val_loss: 1.8501 - val_accuracy: 0.4688\n",
            "Epoch 49/50\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2018 - accuracy: 0.6100 - val_loss: 1.8775 - val_accuracy: 0.4606\n",
            "Epoch 50/50\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.1960 - accuracy: 0.6119 - val_loss: 1.8558 - val_accuracy: 0.4641\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f0415d172d0>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FhV-fVzcNc3p"
      },
      "source": [
        "## Question 9\n",
        "\n",
        "What is the difference between a Multi-class and Multi-label Classification problem, and what sort of loss function would we need to learn them?\n",
        "\n",
        "\\<*Type your answer here*\\>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITEN1-3-OQwd"
      },
      "source": [
        "## Question 10\n",
        "What is the relationship between Binary Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "\\<*Type your answer here*\\>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uGwk3Z0xOmRk"
      },
      "source": [
        "## Question 11\n",
        "\n",
        "What is the relationship between Sparse Cross entropy and Categorical Cross entropy?\n",
        "\n",
        "\\<*Type your answer here*\\>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2RDBtUuYjuFh"
      },
      "source": [
        "# **Upload this Day 9 Colab Notebook to your Github repository under \"Day 9\" folder. Also add your *Reflection* on today's learning in README.md**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgg0bvRjS9un"
      },
      "source": [
        "Sources:\n",
        "\n",
        "https://towardsdatascience.com/common-loss-functions-in-machine-learning-46af0ffc4d23\n",
        "\n",
        "https://peltarion.com/knowledge-center/documentation/modeling-view/build-an-ai-model/loss-functions/mean-squared-logarithmic-error-(msle)"
      ]
    }
  ]
}